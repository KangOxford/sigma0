<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Scaling Laws for Quantitative Finance Foundation Models - LOBS5">
    <meta name="keywords" content="foundation model, limit order book, state space model, order flow, quantitative finance, scaling laws">
    <title>LOBS5 - Scaling Laws for Quantitative Finance Foundation Models</title>
    <link rel="icon" type="image/png" href="imgs/favicon.png">
    <link rel="apple-touch-icon" href="imgs/favicon.png">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- CodeMirror (for BibTeX display) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <!-- Template styles -->
    <link rel="stylesheet" href="utils/style.css">
    <!-- Custom styles -->
    <link rel="stylesheet" href="style.css">
    <style>
        /* Sponsor grid: always 2Ã—2 */
        .sponsor-grid { display: grid; gap: 15px; grid-template-columns: 1fr 1fr; }
    </style>

    <!-- MathJax 3 -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">
        <div class="col-md-10 col-md-offset-1">

            <!-- ========== TITLE ========== -->
            <div class="text-center" style="margin-top: 40px;">
                <h1 style="font-size: 48px; font-weight: 700; font-family: 'Georgia', 'Times New Roman', serif;">
                    <em>&sigma;<sub>0</sub></em>: Unlock the Large Scale Quant Foundation Models
                </h1>
            </div>

            <!-- ========== AUTHORS (dynamically shuffled) ========== -->
            <div class="text-center" style="margin-top: 20px;">
                <div id="author-list" style="font-size: 15px; line-height: 2.2em;"></div>
                <div id="faculty-list" style="font-size: 15px; line-height: 2.2em; margin-top: 8px;"></div>
                <p id="seed-note" style="font-size: 12px; color: #999; margin-top: 8px;"></p>
            </div>

            <!-- ========== AFFILIATIONS (logos) ========== -->
            <div class="text-center" style="margin-top: 15px; margin-bottom: 35px;">
                <img src="imgs/oxford_logo.png" alt="University of Oxford" height="48" style="display: inline-block; margin: 0 20px; vertical-align: middle;">
                <img src="imgs/flair_logo.png" alt="FLAIR" height="48" style="display: inline-block; margin: 0 20px; vertical-align: middle;">
            </div>

            <!-- ========== NAV BUTTONS (single row) ========== -->
            <div class="text-center" style="margin-top: 20px; margin-bottom: 30px; display: flex; flex-wrap: nowrap; justify-content: center; gap: 8px;">
                <button onclick="toggleNav('papers-list')" style="padding: 10px 18px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; background: #fafafa; cursor: pointer; text-align: center;">
                    <i class="fa fa-file-text-o" style="margin-right: 6px;"></i>Papers
                </button>
                <button onclick="toggleNav('code-list')" style="padding: 10px 18px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; background: #fafafa; cursor: pointer; text-align: center;">
                    <i class="fa fa-github" style="margin-right: 6px;"></i>Code
                </button>
                <button onclick="toggleNav('checkpoint-list')" style="padding: 10px 18px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; background: #fafafa; cursor: pointer; text-align: center;">
                    <i class="fa fa-database" style="margin-right: 6px;"></i>Checkpoint
                </button>
                <button onclick="toggleNav('past-contrib-list')" style="padding: 10px 18px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; background: #fafafa; cursor: pointer; text-align: center;">
                    <i class="fa fa-users" style="margin-right: 6px;"></i>Past Contributors
                </button>
            </div>

            <!-- Nav dropdowns (mutually exclusive, below buttons) -->
            <div id="papers-list" class="nav-dropdown" style="display: none; margin-bottom: 20px; text-align: left; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px 20px; background: #fafafa;">
                    <table style="width: 100%; border: none;">
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2309.00638" style="font-size: 14px; font-weight: 600;">Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network</a><br>
                                <span style="font-size: 12px; color: #666;">P Nagy, S Frey, S Sapora, K Li, A Calinescu, S Zohren, J Foerster</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Oral] ICAIF 2023</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 34</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2308.13289" style="font-size: 14px; font-weight: 600;">JAX-LOB: A GPU-Accelerated Limit Order Book Simulator to Unlock Large Scale Reinforcement Learning for Trading</a><br>
                                <span style="font-size: 12px; color: #666;">SY Frey, K Li, P Nagy, S Sapora, C Lu, S Zohren, J Foerster, A Calinescu</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Best Paper] ICAIF 2023</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 34</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2502.09172" style="font-size: 14px; font-weight: 600;">LOB-Bench: Benchmarking Generative AI for Finance: an Application to Limit Order Book Data</a><br>
                                <span style="font-size: 12px; color: #666;">P Nagy, S Frey, K Li, B Sarkar, S Vyetrenko, S Zohren, A Calinescu, ...</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">ICML 2025</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 5</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://dl.acm.org/doi/10.1145/3677052.3698691" style="font-size: 14px; font-weight: 600;">Mixtures of Experts for Scaling Up Neural Networks in Order Execution</a><br>
                                <span style="font-size: 12px; color: #666;">K Li, M Cucuringu, L S&aacute;nchez-Betancourt, T Willi</span><br>
                                <span style="font-size: 12px; color: #999;">ICAIF 2024 &nbsp;&middot;&nbsp; Cited: 5</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://dl.acm.org/doi/10.1145/3768292.3770442" style="font-size: 14px; font-weight: 600;">Discrete Flow Matching is a Surprisingly Effective Post-training Method to Address Compound Error in Autoregressive Models</a><br>
                                <span style="font-size: 12px; color: #666;">K Li, B Sarkar, Z Xiong, S Frey, Z Wang, F Zejnullahu, A Backhouse, ...</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Oral] ICAIF 2025</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 1</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://dl.acm.org/doi/pdf/10.1145/3768292.3770416" style="font-size: 14px; font-weight: 600;">JaxMARL-HFT: GPU-Accelerated Large-Scale Multi-Agent Reinforcement Learning for High-Frequency Trading</a><br>
                                <span style="font-size: 12px; color: #666;">V Mohl, S Frey, R Leyland, K Li, G Nigmatulin, M Cucuringu, S Zohren, ...</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Best Paper] ICAIF 2025</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 1</span>
                            </td>
                        </tr>
                        <tr>
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2509.05107" style="font-size: 14px; font-weight: 600;">Painting the Market: Generative Diffusion Models for Financial Limit Order Book Simulation and Forecasting</a><br>
                                <span style="font-size: 12px; color: #666;">A Backhouse, K Li, J Foerster, A Calinescu, S Zohren</span><br>
                                <span style="font-size: 12px; color: #999;">arXiv 2025</span>
                            </td>
                        </tr>
                    </table>
                </div>

            <!-- Code dropdown -->
            <div id="code-list" class="nav-dropdown" style="display: none; margin-bottom: 20px; text-align: center; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px 20px; background: #fafafa;">
                    <p style="margin: 0;"><a href="https://github.com/KangOxford/sigma0" style="font-size: 14px;"><i class="fa fa-github"></i>&nbsp; github.com/KangOxford/sigma0</a></p>
                </div>

            <!-- Checkpoint dropdown -->
            <div id="checkpoint-list" class="nav-dropdown" style="display: none; margin-bottom: 20px; text-align: center; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px 20px; background: #fafafa;">
                    <p style="margin: 0; color: #777;">Checkpoints coming soon.</p>
                </div>

            <!-- Past contributors dropdown -->
            <div id="past-contrib-list" class="nav-dropdown" style="display: none; margin-bottom: 20px;">
                <p style="font-size: 13px; color: #777; text-align: center; margin: 8px 0;">
                    <a href="https://www.linkedin.com/in/peer-nagy/" style="color:#777;">Peer Nagy</a> &middot; <a href="https://www.linkedin.com/in/alfred-backhouse-12540a166/" style="color:#777;">Alfie Backhouse</a> &middot; <a href="https://www.linkedin.com/in/reubenleyland/" style="color:#777;">Reuben</a> &middot; <a href="https://www.linkedin.com/in/frensi-zejnullahu" style="color:#777;">Frensi Zejnullahu</a> &middot; <a href="https://www.linkedin.com/in/chris-lu-37471b119/" style="color:#777;">Chris Lu</a> &middot; <a href="https://www.linkedin.com/in/gereon-franken/" style="color:#777;">Gereon Franken</a> &middot; <a href="https://www.linkedin.com/in/silvia-sapora/" style="color:#777;">Silvia Sapora</a> &middot; <a href="https://www.linkedin.com/in/timon-willi/" style="color:#777;">Timon Willi</a>
                </p>
            </div>

            <!-- ========== HERO IMAGE ========== -->
            <div id="header_img">
                <img src="imgs/scaling_law.png" alt="Scaling Laws for LOB Foundation Models" style="width: 90%;">
                <p class="fig-caption">
                    <strong>Figure 1.</strong> Loss vs. model parameters, trained on 25 billion tokens of NASDAQ order flow. Test loss decreases monotonically from 34M to 617M parameters, following a power-law relationship. The ratio indicates tokens per parameter.
                </p>
            </div>

            <!-- ========== ABSTRACT ========== -->
            <blockquote onclick="var el=document.getElementById('abstract-full');el.style.display=el.style.display==='none'?'block':'none';" style="cursor:pointer; background-color:#f0f4ff; border:1px solid #c0d0f0; border-radius:12px; padding:20px 25px; margin:0;">
            <p>
                <strong>TL;DR</strong>: Finance is one of the hardest domains for ML: noisy, fast, strategic. We develop <strong>LOBS5</strong>, the first state-space foundation model for limit order book data, trained on <strong>25 billion tokens</strong> of NASDAQ order flow. Our three-stage pipeline (pre-training, post-training with Discrete Flow Matching, fine-tuning with Evolution Strategies) achieves <strong>155% PnL improvement</strong> on order execution and exhibits clear scaling laws from 34M to 2B+ parameters.
            </p>
            <div id="abstract-full" style="display:none; text-align:left;">
                    <hr style="border: none; border-top: 1px dashed #b0c4de; margin: 12px 0;">
                    <h3 style="margin: 8px 0 12px; font-size: 16px; font-weight: 700;">Abstract</h3>
                    <p>
                        Financial markets operate through limit order books (LOBs), which aggregate market participants' trading intentions in the form of limit orders specifying order type, direction, price, and quantity. The continuous stream of these orders, the <em>order flow</em>, provides a high-fidelity record of market dynamics at unprecedented scale: the S&amp;P 500 constituents alone generated approximately <strong>3.8 trillion tokens</strong> of order flow between 2016 and 2021, comparable to the largest natural language corpora.
                    </p>
                    <p>
                        We introduce <strong>LOBS5</strong>, a foundation model for LOB data built on the S5 State Space Model architecture, pretrained on 25 billion tokens from NASDAQ using a 24-token encoding scheme that discretizes each limit order message into categorical tokens. Unlike natural language where tokens are purely symbolic, order flow comprises both categorical values (event type, direction) and numerical values (price, quantity) in which magnitude carries semantic meaning, making LOB order flow an ideal testbed for foundation models on structured time series.
                    </p>
                    <p>
                        Our complete training pipeline consists of three stages: (1) S5-based pre-training with <em>Sliding Window Recurrences</em> for long context processing, (2) <em>Discrete Flow Matching</em> post-training that eliminates compound error through bidirectional refinement (48.6% reduction in Wasserstein distance), and (3) <em>Evolution Strategies</em> fine-tuning via EGGROLL that achieves 155% PnL improvement on order execution tasks with 65,536 parallel simulations. Pre-training exhibits clear scaling laws: test loss decreases monotonically from 34M to 617M parameters, achieving approximately 40% Model FLOPs Utilization comparable to Meta's Llama-3.1.
                    </p>
            </div>
            </blockquote>

            <!-- ========== PHILOSOPHY ========== -->
            <button class="collapsible">Philosophy</button>
            <div class="contentx">
                <p>
                    We treat quantitative finance as a <strong>data science problem</strong>. Rather than hand-crafting trading rules or relying on closed-form models, we build <strong>AI infrastructure</strong>, foundation models, simulators, and evaluation pipelines, and then <strong>iterate rapidly</strong> through experiments.
                </p>
                <p>
                    The core loop is: <strong>build &rarr; experiment &rarr; measure &rarr; iterate</strong>. We train models on terabytes of order flow data, run thousands of parallel simulations, compare results quantitatively, and feed insights back into the next iteration. Every design decision is validated empirically, not assumed theoretically.
                </p>
                <p>
                    Because it is science: hypotheses are tested against data, results must be reproducible, and the infrastructure must support fast turnaround from idea to experiment to conclusion. This is why we invest heavily in GPU-accelerated simulation, automated benchmarking (LOB-Bench), and scalable training pipelines, they are the laboratory equipment that makes rapid experimental iteration possible.
                </p>
                <p>
                    A core design principle is to <strong>stay true to the data</strong>. We discretize every field of an order message into categorical tokens and train with <strong>cross-entropy loss</strong>, performing general density modeling without assuming any parametric distribution. Regression losses and continuous distribution matching require distributional assumptions (Gaussian, exponential, etc.) that may not hold for financial data. Cross-entropy over discrete tokens is the only approach that models arbitrary distributions faithfully, letting the data speak for itself.
                </p>
            </div>

            <!-- ========== TERMINOLOGY ========== -->
            <button class="collapsible">Terminology</button>
            <div class="contentx">
                <table class="data-table" style="max-width: 600px; margin: 12px auto 0;">
                    <thead>
                        <tr><th style="width: 20%;">Name</th><th style="width: 25%;">What it is</th><th>Analogy</th></tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>&sigma;<sub>0</sub></strong></td>
                            <td>Model family</td>
                            <td>Like <em>LLaMA</em> or <em>GPT</em>. A herd of models including pre-trained foundation models and fine-tuned models for downstream tasks (order execution, market making, etc.). The core is the pre-trained foundation model; &sigma;<sub>0</sub> is architecture-agnostic and future releases may use Transformers, Mamba, or hybrid architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>LOBS5</strong></td>
                            <td>Architecture</td>
                            <td>Like <em>Transformer</em> or <em>Mamba</em>. A specific architecture built on S5 (Simplified Structured State Space) layers that replaces attention with linear-time state space recurrences. LOBS5 is the architecture used in the current &sigma;<sub>0</sub> release.</td>
                        </tr>
                    </tbody>
                </table>
                <p style="font-size: 13px; color: #666; text-align: center; margin-top: 10px;">
                    In short: <strong>&sigma;<sub>0</sub></strong> is <em>what</em> we release, <strong>LOBS5</strong> is <em>how</em> it is built.
                </p>
            </div>

            <!-- ========== WHY ORDER FLOW ========== -->
            <h2>Why Order Flow?</h2>
            <p>
                The stock exchanges (NYSE and NASDAQ) generate massive volumes of high-frequency trading data, with new orders created at the millisecond level. These orders encapsulate a wealth of insights: the tendencies and preferences of various market participants, underlying market information, and the correlations between different stocks.
            </p>
            <p>
                Our core objective is to train a foundation model on this terabyte-scale dataset, enabling it to learn the transition dynamics of limit order books. The pipeline has three distinct stages: <strong>pre-training</strong> learns the data distribution via next-token prediction, <strong>post-training</strong> (Discrete Flow Matching) improves generation quality by eliminating compound error, and <strong>fine-tuning</strong> (Evolution Strategies) optimizes for downstream tasks such as order execution. Even a few basis points of improvement in execution can translate to enormous profits when deployed at scale.
            </p>

            <button class="collapsible">The Multi-Agent Perspective</button>
            <div class="contentx">
                <p style="margin-top: 12px;">
                    At its core, a limit order book is an <strong>anonymous multi-agent system</strong>. Each market participant submits orders (actions) that modify the shared order book (state). The <em>state transitions</em> are governed by the matching engine (deterministic), but the <em>distribution of future states</em> depends on the collective behavior of all participants, analogous to video frame prediction in computer vision.
                </p>
                <p>
                    LOBS5 captures this state distribution through next-token prediction on order flow tokens. The foundation model learns two fundamental capabilities:
                </p>
                <ul>
                    <li><strong>Mimic historical orders</strong>: Generate realistic background market activity as a world model</li>
                    <li><strong>Optimize trading policy</strong>: Fine-tune an ego agent for downstream tasks using evolution strategies</li>
                </ul>
                <p>
                    This enables simulation of <em>dynamic price impact</em> (not static), <em>indirect price impact</em> (not direct), and <em>adversarial selection</em>, where other agents observe and react to the ego agent's behavior, changing their original plans. These phenomena are critical for realistic market simulation but are absent from traditional backtesting frameworks.
                </p>
            </div>

            <!-- ========== PRE-TRAINING ========== -->
            <h2>LOBS5: Pre-training at Scale</h2>

            <h3>Architecture: S5 State Space Model</h3>
            <p>
                LOBS5 is built on the <strong>S5 State Space Model</strong>, a modern alternative to Transformers that provides linear-time $O(n)$ complexity for sequence modeling, compared to the quadratic $O(n^2)$ cost of self-attention. The choice of S5 is motivated by three factors:
            </p>
            <img src="imgs/lobs5_architecture.png" alt="LOBS5 Architecture: S5 Message Module, S5 Book Module, Combined S5 Module" style="width: 95%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 2.</strong> LOBS5 architecture. The S5 Message Module processes masked message sequences, while the S5 Book Module processes order book snapshots. Their outputs are concatenated and fed into the Combined S5 Module, which produces next-token probabilities via mean-pooling and softmax.
            </p>

            <ol>
                <li><strong>Linear complexity</strong>: Efficient processing of long order flow sequences (10,000+ tokens)</li>
                <li><strong>Natural bridge</strong>: The state space formulation naturally connects continuous market dynamics with discrete observations</li>
                <li><strong>Memory efficiency</strong>: 40&ndash;60% lower memory usage compared to Transformers</li>
            </ol>
            <p>
                A key open question is whether LOB modeling requires the full quadratic attention mechanism, or whether linear attention with longer context windows is sufficient. Our scaling results suggest the latter, LOBS5 achieves strong performance with linear complexity, and performance continues to improve with model scale.
            </p>

            <h3>Long Context: What Does 10k Orders Mean?</h3>
            <p>
                With a TBPTT window of 20 segments (each 512 tokens), LOBS5 processes approximately <strong>10,000 orders</strong> per training context. But how much real market time does that represent? The answer depends heavily on market activity:
            </p>
            <img src="imgs/context_window_time.png" alt="10k messages wall-clock time across years and activity levels" style="width: 85%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 3.</strong> Wall-clock time spanned by 10k order messages (GOOG, 2016&ndash;2023). High-activity periods (red) compress 10k orders into just 2&ndash;5 minutes, while low-activity periods (gray) can stretch to 30+ minutes. The secular trend toward higher activity means recent data packs more information per context window.
            </p>

            <h3>24-Token Encoding</h3>
            <p>
                Each limit order message is discretized into <strong>24 categorical tokens</strong> representing:
            </p>
            <ul>
                <li><strong>Event type</strong>: new order, cancellation, execution, etc.</li>
                <li><strong>Direction</strong>: buy or sell</li>
                <li><strong>Relative price levels</strong>: distance from best bid/ask</li>
                <li><strong>Size digits</strong>: order quantity decomposed into individual digits</li>
            </ul>
            <p>
                This encoding enables the application of language modeling techniques (next-token prediction with cross-entropy loss) to financial time series while preserving the semantic meaning of numerical values.
            </p>

            <h3>Why Lossless Tokenization?</h3>
            <p>
                Alternative approaches such as <a href="https://arxiv.org/abs/2405.01806">MarS</a> and <a href="https://arxiv.org/abs/2408.01441">LOBERT</a> compress each message into a single token (lossy), producing much shorter sequences (500 tokens vs. our 12,000 for 500 messages). We deliberately choose lossless encoding for two reasons:
            </p>
            <p>
                <strong>1. The simulator requires exact messages.</strong> Our full pipeline runs generated messages through the <a href="https://arxiv.org/abs/2308.13289">JAX-LOB</a> matching engine, which implements price-time priority matching. This requires exact prices (to determine queue position), exact sizes (to compute partial fills), and exact event types (new order vs. cancellation have completely different logic). Lossy encoding would break the matching engine or produce incorrect trade executions, making the entire fine-tuning loop (RL or EGGROLL) impossible.
            </p>
            <p>
                <strong>2. S5 makes long sequences affordable.</strong> For Transformers, 12,000 tokens costs $O(n^2) = 144M$ attention computations, which is why lossy approaches are attractive. But S5 processes sequences in $O(n)$, making the 24&times; sequence length overhead manageable: our S5 model trains in 2.5 hours per epoch vs. 20 hours for an equivalent Transformer. Lossless encoding is a viable choice precisely because the architecture can handle it.
            </p>

            <h3>GPU Efficiency</h3>
            <p>
                We achieve approximately <strong>40% Model FLOPs Utilization</strong> (MFU) during pre-training, comparable to Meta's Llama-3.1 (38&ndash;43%):
            </p>

            <table class="data-table" style="max-width: 450px; margin: 0 auto;">
                <thead>
                    <tr>
                        <th>Model Size</th>
                        <th>MFU</th>
                        <th>GPU Util %</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>2.17B</td><td>38.4%</td><td>93.4 &plusmn; 7.9</td></tr>
                    <tr><td>1.95B</td><td>35.3%</td><td>95.6 &plusmn; 7.4</td></tr>
                    <tr><td>1.86B</td><td>36.3%</td><td>94.2 &plusmn; 7.3</td></tr>
                    <tr><td>1.4B</td><td>41.7%</td><td>80.7 &plusmn; 37.5</td></tr>
                    <tr><td>1.0B</td><td>41.3%</td><td>72.9 &plusmn; 41.3</td></tr>
                    <tr><td>360M</td><td>36.2%</td><td>85.0 &plusmn; 33.9</td></tr>
                </tbody>
            </table>
            <p style="font-size: 13px; color: #666; text-align: center;">
                All metrics measured on a single node (4&times; GH200 GPUs).
            </p>

            <h3>Throughput</h3>
            <img src="imgs/throughput.png" alt="Message Generation Throughput" style="width: 85%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 4.</strong> Message generation throughput comparison. LOBSv2 achieves <strong>100&ndash;600&times;</strong> throughput improvement over v1, reaching up to 15,000 messages/second.
            </p>

            <h3>Generation Quality Across Versions</h3>
            <p>
                Successive iterations of LOBS5 show consistent improvement in generation fidelity. Evaluated on both INTC and GOOG order flow, LOBS5v2 (conditional and unconditional) substantially outperforms v1 across L1 and Wasserstein distance metrics, with lower values indicating closer distributional match to real market data:
            </p>
            <img src="imgs/lobs5_versions.png" alt="LOBS5 version comparison: L1 and Wasserstein distances" style="width: 85%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 4.</strong> L1 and Wasserstein distance across LOBS5 versions (v1, v2-conditional, v2-unconditional) on INTC and GOOG. Each version progressively reduces distributional distance, with v2 variants achieving roughly <strong>2&times;</strong> improvement over v1.
            </p>

            <!-- ========== POST-TRAINING ========== -->
            <h2>Post-training: Discrete Flow Matching</h2>
            <p>
                A fundamental limitation of autoregressive (AR) models is <strong>compound error accumulation</strong>: prediction errors propagate and amplify throughout sequence generation, as each token conditions on potentially incorrect previous outputs. This problem is particularly severe for financial order flow, where distributional shifts compound rapidly over long generation horizons.
            </p>
            <p>
                To address this, we introduce a post-training stage using <strong>Discrete Flow Matching (DFM)</strong>, which converts our pretrained AR model into a flow-based generator. Unlike DDPM that starts from random noise, our approach leverages the pretrained model's learned distribution as the starting point, significantly reducing the learning burden. Compared to GANs, which we found unstable and only marginally better than baselines on LOB data, DFM trains reliably and consistently converges.
            </p>
            <p>
                Crucially, DFM does not replace cross-entropy pre-training: <strong>CE learns <em>what</em> to generate</strong> (the data distribution), while <strong>DFM improves <em>how</em> to generate</strong> (the sampling procedure).
            </p>

            <!-- ========== SIMULATOR ========== -->
            <h2>Simulator</h2>
            <p>
                Training and evaluating trading agents requires a fast, faithful market simulator. Our simulator stack has three layers, each adding richer dynamics:
            </p>

            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #fff8f0; border: 1px solid #f0dcc0;">
                <h3 style="margin-top: 0;">Layer 1: Match Engine: JAX-LOB</h3>
                <p style="margin-bottom: 0;">
                    At the foundation is <a href="https://arxiv.org/abs/2308.13289"><strong>JAX-LOB</strong></a> (<span style="color: #e74c3c; font-weight: 700;">ICAIF 2023 Best Paper</span>), a fully vectorized limit order book matching engine in JAX. It implements the complete NASDAQ ITCH protocol with price-time priority matching. Given an incoming order and the current book state, the match engine deterministically executes trades and updates the book.
                </p>
            </div>

            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #fff3eb; border: 1px solid #f0cdb8;">
                <h3 style="margin-top: 0;">Layer 2: Environment: Historical Replay</h3>
                <p>
                    Wrapping the match engine with historical order flow replay creates an <strong>environment</strong>. The agent's orders are interleaved with real historical messages and processed through the match engine. This layer captures <strong>direct price impact</strong>, when your order consumes liquidity at a price level, that liquidity is gone and cannot be filled again. However, it has a critical limitation: the historical background agents do not react to the ego agent's actions, they replay the same orders regardless of what the agent does.
                </p>
                <p style="margin-bottom: 0;">
                    <a href="https://dl.acm.org/doi/pdf/10.1145/3768292.3770416"><strong>JaxMARL-HFT</strong></a> (<span style="color: #e74c3c; font-weight: 700;">ICAIF 2025 Best Paper</span>) extends this to multi-agent settings, where multiple trading agents interact within the same order book. Both simulators leverage JAX's <code>vmap</code> to run <strong>65,536 parallel environments</strong> on a single GPU cluster (2,048 per GPU &times; 32 GPUs).
                </p>
            </div>

            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #ffede6; border: 1px solid #f0bfaf;">
                <h3 style="margin-top: 0;">Layer 3: World Model Simulator: LOBS5 as Background</h3>
                <p>
                    The full simulator replaces historical replay with a <strong>world model</strong>: the frozen pretrained LOBS5 generates background market activity <em>conditioned on the current state</em>, including the ego agent's actions. This unlocks <strong>indirect price impact</strong>, phenomena that historical replay cannot capture:
                </p>
                <ul>
                    <li><strong>Adversarial selection</strong>: Background agents observe the ego agent's orders and change their behavior accordingly. For example, a market maker seeing a large buy order may widen their spread or pull their quotes, exactly the adverse selection that makes real trading difficult</li>
                    <li><strong>Dynamic liquidity response</strong>: The order book replenishes (or thins out) in response to the ego agent's activity, rather than following a fixed historical script</li>
                    <li><strong>Strategic interaction</strong>: The world model learns the <em>equilibrium distribution</em> of market responses, providing a far more realistic training signal than static replay</li>
                </ul>
                <p style="margin-bottom: 0;">
                    This creates a two-model architecture: the <strong>policy model</strong> (fine-tuned LOBS5) learns to trade, while the <strong>world model</strong> (frozen pretrained LOBS5) provides the reactive environment. The policy model must learn strategies that are robust to adversarial selection, not just strategies that work against passive historical data.
                </p>
            </div>

            <h3>Order Injection: Validating Price Impact</h3>
            <p>
                To verify that the world model captures realistic price impact, we inject buy or sell market orders at the junction point (step 500) and observe the midprice response:
            </p>
            <img src="imgs/injection_orders.png" alt="Midprice return after order injection: BUY vs SELL" style="width: 90%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 7.</strong> Aggregated midprice return (mean &plusmn; 1 std) after injecting BUY (blue) and SELL*(&minus;1) (red) orders at the junction. The combined mean (purple) shows a clear positive drift post-injection, confirming that the world model produces realistic indirect price impact, background agents respond to the injected orders rather than ignoring them.
            </p>

            <h3>Triton Kernel Fusion</h3>
            <p>
                The match engine (Layer 1) is the performance bottleneck. Order book matching is inherently <strong>sequential</strong>: each trade modifies the book state, and the next match depends on the updated state. In JAX, <code>lax.while_loop</code> requires a <strong>device-to-host synchronization</strong> on every iteration, costing 110&ndash;500ms per match despite the entire book fitting in 4.8 KB of SRAM.
            </p>
            <p>
                We rewrite the matching engine as fused <strong>Triton kernels</strong> that keep the entire while loop on-GPU:
            </p>
            <ul>
                <li><strong>Zero D2H overhead</strong>: Loop condition evaluated in SRAM, no CPU synchronization needed</li>
                <li><strong>99% HBM efficiency</strong>: Order book data (2.4 KB per side) pinned in shared memory for the entire kernel duration</li>
                <li><strong>Fused operations</strong>: Price-time priority search, quantity deduction, trade recording, and empty-slot cleanup, all in a single kernel launch</li>
                <li><strong>Sequential correctness</strong>: The kernel preserves exact price-time priority semantics via an in-kernel for-loop</li>
            </ul>

            <!-- ========== FINE-TUNING ========== -->
            <h2>Fine-tuning for Trading</h2>
            <p>
                Foundation models must adapt to domain-specific tasks. LOBS5 can be fine-tuned for a range of trading problems: <strong>order execution</strong> (executing a specified quantity within a time window while minimizing market impact), <strong>market making</strong> (continuously quoting bid/ask prices to earn the spread while managing inventory risk), and <strong>statistical arbitrage</strong> (exploiting transient mispricings across correlated instruments). Two fundamentally different optimization paradigms apply:
            </p>

            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #f0f4ff; border: 1px solid #c0d0f0;">
                <h3 style="margin-top: 0;">Approach 1: Reinforcement Learning (First-Order)</h3>
                <p>
                    Gradient-based fine-tuning via <strong>backpropagation</strong>: both forward pass and backward pass are required. This includes Inverse Reinforcement Learning (IRL) and standard RL policy gradient methods. Constraints must be expressed as differentiable regularisation terms in the loss function, limiting them to simple mathematical forms (L2 penalties, KL divergence bounds).
                </p>
                <p style="margin-bottom: 0;">
                    This approach is well-suited when the reward signal is differentiable or can be approximated by a learned reward model, but struggles with non-differentiable objectives like realized PnL.
                </p>
            </div>

            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #f0fff4; border: 1px solid #c0f0c8;">
                <h3 style="margin-top: 0;">Approach 2: Evolution Strategies (Zeroth-Order)</h3>
                <p>
                    <strong>EGGROLL</strong> (Evolution Guided GeneRal Optimisation via Low-rank Learning) <strong>converts training into inference</strong>: no backpropagation is needed. Instead, the model is evaluated with random noise perturbations $AB^\top$ (LoRA rank-4) on all projection matrices, and the fitness signal (realized PnL) guides the search. This reduces memory from $mn$ to $r(m+n)$ per layer and achieves up to a <strong>hundredfold increase</strong> in training throughput for billion-parameter models.
                </p>
                <p>Two key advantages over gradient-based methods:</p>
                <ul>
                    <li><strong>Arbitrary constraints</strong>: Rather than encoding constraints as differentiable loss terms, Evolution Strategies can enforce complex, semantically described constraints (e.g., "never hold more than X shares", "respect exchange rate limits", "maintain inventory within bounds") by simply rejecting or penalising violating rollouts. The constraints do not need to be differentiable.</li>
                    <li><strong>Stronger exploration</strong>: Each generation samples diverse noise perturbations across the entire parameter space, naturally exploring multiple modes of the fitness landscape. Unlike gradient descent, which concentrates updates around the current mean, ES does not collapse to the nearest local optimum but maintains broad coverage of the solution space.</li>
                </ul>
            </div>

            <h3>Results: Order Execution</h3>

            <!-- RL results (blue box) -->
            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #f0f4ff; border: 1px solid #c0d0f0;">
                <h4 style="margin-top: 0;">Approach 1 Result: Reinforcement Learning</h4>
                <img src="imgs/rl_finetuning.png" alt="RL fine-tuning: Reward and PnL vs Episode for VOD.L" style="width: 75%; margin: 15px auto;">
                <p class="fig-caption">
                    <strong>Figure 7.</strong> RL fine-tuning using SARSA (State-Action-Reward-State-Action, an on-policy temporal-difference method) on VOD.L (Vodafone, London Stock Exchange) order execution. <strong>Top</strong>: Reward vs. episode, showing convergence from negative to ~1.8M over 1,000 episodes. <strong>Bottom</strong>: PnL vs. episode, rising from ~1.2&times;10<sup>4</sup> to ~2.5&times;10<sup>4</sup>. Orange/blue lines show 10-episode rolling averages.
                </p>
            </div>

            <!-- EGGROLL results (green box) -->
            <div style="border-radius: 12px; padding: 20px 25px; margin-bottom: 15px; background-color: #f0fff4; border: 1px solid #c0f0c8;">
                <h4 style="margin-top: 0;">Approach 2 Result: EGGROLL</h4>
                <p>
                    The task is to execute a sell order of <strong>Q = 30 shares</strong> within a horizon of <strong>T = 10 steps</strong>. In each episode, the LOB is initialised from a snapshot followed by 10 warm-up background messages. At each step, the population members generate their messages, followed by 50 real background messages processed through the <a href="https://arxiv.org/abs/2308.13289">JAX-LOB</a> simulator. Fine-tuning is performed on GOOG order flow from January 2023, applying LoRA (rank 4) on all projection matrices while freezing SSM parameters and layer norms. Performance is measured by realized PnL relative to the initial mid price.
                </p>

                <img src="imgs/eggroll_training.png" alt="EGGROLL training curves: Mean PnL and PnL Std" style="width: 85%; margin: 15px auto;">
                <p class="fig-caption">
                    <strong>Figure 8.</strong> Training curves for order execution with EGGROLL. <strong>Left</strong>: Mean PnL over 6,500 epochs for baseline (&sigma;=0, orange dashed) and EGGROLL (&sigma;=0.01, blue solid). <strong>Right</strong>: PnL standard deviation. The initial increase in std corresponds to an exploration phase; the subsequent decrease indicates convergence to a high-performing policy. Shaded regions show interquartile range.
                </p>

                <table class="data-table" style="max-width: 550px; margin: 0 auto;">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>Model</td><td>LOBS5-360M</td></tr>
                        <tr><td>Mean PnL (pretrained baseline)</td><td>~4,700</td></tr>
                        <tr><td>Mean PnL (EGGROLL fine-tuned)</td><td><strong>~12,000</strong></td></tr>
                        <tr><td>Improvement</td><td class="metric-highlight">155%</td></tr>
                        <tr><td>Parallel generations (total)</td><td>65,536</td></tr>
                        <tr><td>Parallel generations per GPU</td><td>2,048</td></tr>
                        <tr><td>LoRA rank</td><td>4</td></tr>
                        <tr><td>&sigma; (noise scale)</td><td>0.01</td></tr>
                        <tr><td>Learning rate</td><td>0.001</td></tr>
                        <tr><td>Epochs</td><td>6,500</td></tr>
                    </tbody>
                </table>
                <p style="margin-top: 15px; margin-bottom: 0;">
                    The baseline policy (&sigma;=0, pretrained model without noise perturbation) achieves a mean PnL of approximately 4,700. EGGROLL fine-tuning improves this to around 12,000, a <strong class="metric-highlight">155% improvement</strong>. The PnL standard deviation initially increases to ~3,100 during the first 2,500 epochs (exploration phase), then decreases to ~400 by epoch 6,500, indicating convergence to a concentrated, high-performing policy.
                </p>
            </div>

            <!-- ========== BENCHMARKING ========== -->
            <h2>Benchmarking: LOB-Bench</h2>
            <p>
                How do we know a generative model is producing realistic order flow? We develop <a href="https://arxiv.org/abs/2502.09172"><strong>LOB-Bench</strong></a> (<em>ICML 2025</em>), a comprehensive benchmarking suite that evaluates generative LOB models across multiple dimensions:
            </p>
            <ul>
                <li><strong>Distributional fidelity</strong>: L1 and Wasserstein distance on price returns, spread, and queue sizes</li>
                <li><strong>Temporal structure</strong>: Inter-arrival time distributions, autocorrelation of order flow</li>
                <li><strong>Market microstructure</strong>: Price impact functions, order book shape recovery, volatility clustering</li>
                <li><strong>Cross-model comparison</strong>: Standardised evaluation of autoregressive, diffusion, and flow-matching approaches</li>
            </ul>
            <p>
                LOB-Bench provides the first apples-to-apples comparison framework for this domain, enabling reproducible evaluation as models scale.
            </p>

            <h3>Market Impact: Square Root Law</h3>
            <p>
                A critical test of any order flow model is whether it reproduces the <strong>square root law of market impact</strong>, a universal empirical regularity where price response to order flow scales as $R_\pi \sim l^{1/2}$ with event lag $l$. We compare microscopic response functions between real GOOG data and three LOBS5 variants:
            </p>
            <img src="imgs/market_impact.png" alt="Microscopic response functions: Real Data vs LOBS5 variants" style="width: 95%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 5.</strong> Tick-normalised microscopic response functions for GOOG. Market orders (MO, red) exhibit the characteristic concave square-root shape, limit orders (LO, green) show negative impact, and cancellations (CA, blue) are near-zero. LOBS5v2 variants closely reproduce the real data patterns, while v1 (s5_main) shows deviations at longer lags.
            </p>

            <!-- ========== SCALING ========== -->
            <h2>Scaling to SPY500</h2>
            <p>
                Having established scaling laws on single-stock data, the next frontier is training on the <strong>full S&amp;P 500</strong> universe. The dataset scales are:
            </p>

            <table class="data-table" style="max-width: 450px; margin: 0 auto;">
                <thead>
                    <tr>
                        <th>Dataset</th>
                        <th>Tokens</th>
                        <th>Orders</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>GOOG 2022</td><td>8.3B</td><td>1/3B</td></tr>
                    <tr><td>GOOG (all years)</td><td>25B</td><td>1B</td></tr>
                    <tr><td><strong>SPY500</strong></td><td><strong>3,800B</strong></td><td><strong>152B</strong></td></tr>
                </tbody>
            </table>

            <p style="margin-top: 15px;">
                Scaling to the full SPY500 dataset with a 2B parameter model requires an estimated <strong>40,000&ndash;160,000 node-hours</strong>, derived from our single-stock training profile (16 node-hours/epoch for GOOG 2022), adjusted for the 456&times; data increase and ~50% multi-node communication efficiency.
            </p>
            <p>
                A multi-stock foundation model would unlock <strong>cross-stock correlation learning</strong>, understanding how order flow in one stock influences another, which is critical for portfolio-level applications.
            </p>

            <!-- ========== OPEN QUESTIONS ========== -->
            <h2>Open Questions</h2>
            <p>
                These are unsolved problems we are actively working on. If you have ideas, insights, or preliminary results, <strong>we'd love to hear from you</strong>. Promising contributions lead to co-authorship on future publications. Click any question to explore the details and leave your thoughts below.
            </p>

            <!-- Q1: Attention -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q1</span> Is Order Flow a Complexity Problem or a Long Context Problem?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        There remains a <strong>fundamental unanswered question</strong>: is order flow modeling so complex that we need <strong>quadratic attention</strong> ($O(n^2)$) to capture intricate pairwise dependencies between events? Or is it <em>simple enough</em> that <strong>linear attention</strong> ($O(n)$) suffices, and the real challenge is just seeing enough history?
                    </p>
                    <p>
                        If the latter is true, then <strong>long context</strong> is what matters: as long as every past order/event is in the context window, a linear model can predict well. This would mean the problem is not about attention complexity but about <em>context length</em>, and architectures like S5 that scale linearly with sequence length would be fundamentally better suited than Transformers, which cannot feasibly attend over 100k+ tokens. Concretely, with TBPTT=20 (10k orders), our context covers roughly <strong>3&ndash;30 minutes</strong> of real market time, is this enough, or do we need hours or full-day context? The optimal length likely depends on the task: execution may need minutes, portfolio optimization may need hours.
                    </p>
                    <p>
                        A related dimension is <strong>tokenisation granularity</strong>. Our 24-token-per-message encoding preserves full fidelity but produces very long sequences. Alternative approaches like <a href="https://arxiv.org/abs/2405.01806">MarS</a> and <a href="https://arxiv.org/abs/2408.01441">LOBERT</a> use 1-token-per-message encodings, lossy, but enabling much longer effective context. A third option is <strong>time-stepped representation</strong> using RoPE with fixed time-step snapshots where each step aggregates multiple events, compressing information at the cost of microstructure details. The optimal trade-off between token fidelity and context length remains open.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q1" data-term="Q1 Is Order Flow a Complexity Problem or a Long Context Problem?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q2: Long Context -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q2</span> How Long Should the Context Window Be?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        With TBPTT=20 (10k orders), our context covers roughly <strong>3&ndash;30 minutes</strong> of real market time. Is this sufficient to capture the dynamics that matter for trading, or do we need hours or even full-day context? Longer windows capture intraday trends and slow-moving signals, but they increase computational cost and may introduce noise. The optimal context length likely depends on the downstream task, execution may need minutes, while portfolio optimization may need hours.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q2" data-term="Q2 How Long Should the Context Window Be?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q3: Tokenization -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q3</span> Tokenized Messages vs. Time-Stepped Representations?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        LOBS5 tokenizes each order message into 24 categorical tokens. An alternative is <strong>time-stepped representation</strong>, e.g., using RoPE (Rotary Position Embedding) with fixed time-step snapshots where each step aggregates multiple events. Tokenized representations preserve full message-level fidelity but produce very long sequences. Time-stepped approaches compress information but may lose critical microstructure details. Which is better for which tasks?
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q3" data-term="Q3 Tokenized Messages vs. Time-Stepped Representations?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q4: Data Scaling -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q4</span> Scale Up Stocks or Scale Up Years?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        For better <strong>test-time generalization</strong>, should we train on more stocks (cross-sectional diversity) or more years of the same stock (temporal diversity)? If train and test data come from the same distribution, scaling more stocks (SPY500) provides diversity across market microstructures, while scaling more years captures diverse market regimes (bull, bear, crisis). The optimal data scaling strategy for foundation model generalization in finance remains an open empirical question.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q4" data-term="Q4 Scale Up Stocks or Scale Up Years?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q5: Market Regime -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q5</span> Memorizing Market Regimes via Mixture of Experts?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Financial markets exhibit distinct <strong>regimes</strong>, volatile vs. calm, trending vs. mean-reverting, crisis vs. normal. A single model must <strong>memorize</strong> all these regimes and switch behavior accordingly. Can a <strong>Mixture of Experts</strong> architecture achieve this by dedicating different experts to different regimes, where each expert memorizes the dynamics of a particular market state? The key questions are: how many distinct regimes need to be memorized, can a router learn to detect regime shifts in real time, and does explicit expert specialization outperform a single large model that must implicitly memorize all regimes in its parameters?
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q5" data-term="Q5 Memorizing Market Regimes via Mixture of Experts?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q6: Arithmetic Intensity -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q6</span> Low Arithmetic Intensity of State Space Models?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        SSMs have inherently <strong>low arithmetic intensity</strong>, the ratio of FLOPs to memory accesses is small, meaning they are memory-bandwidth-bound rather than compute-bound. This underutilizes <strong>Tensor Cores</strong> (designed for dense matrix multiply) and puts pressure on HBM bandwidth. How can we restructure SSM computations to increase arithmetic intensity? Possibilities include fusing more operations into single kernels, batching state updates, or hybrid architectures that mix attention (high intensity) with SSM (low intensity) layers.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q6" data-term="Q6 Low Arithmetic Intensity of State Space Models?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q7: XLA Optimization -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q7</span> Hand-Written Kernels vs. XLA Auto-Optimization?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        JAX's <strong>XLA compiler</strong> is a black box that performs sophisticated graph-level optimizations (operator fusion, layout transformation, memory planning). Hand-written Triton kernels give precise control over memory access patterns, but XLA may find optimizations that humans miss. In practice, hand-optimized kernels are not guaranteed to be faster than XLA's auto-compiled output. When should we invest in custom kernels vs. trusting the compiler? A principled approach requires profiling to identify true bottlenecks before committing to manual optimization.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q7" data-term="Q7 Hand-Written Kernels vs. XLA Auto-Optimization?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q8: Hybrid Attention -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q8</span> Hybrid Attention: Mixing Linear and Softmax Layers?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Rather than choosing purely between quadratic attention and linear attention, recent architectures explore <strong>hybrid designs</strong> that interleave both. <a href="https://arxiv.org/abs/2408.11321"><strong>Jamba</strong></a> (AI21) mixes Transformer and Mamba layers, while <a href="https://arxiv.org/abs/2501.08313"><strong>MiniMax-Text-01</strong></a> (456B parameters, 4M context) uses a striking pattern: every 8 layers, 7 use <strong>Lightning Attention</strong> (linear, $O(n)$) and 1 uses <strong>Softmax Attention</strong> (quadratic, $O(n^2)$), achieving 87.5% linear / 12.5% softmax ratio.
                    </p>
                    <p>
                        The insight is that <strong>linear attention struggles with precise retrieval</strong> (needle-in-a-haystack), while softmax excels at it. Periodically inserting a softmax layer acts as a "calibration point" that recovers fine-grained token lookup without sacrificing the efficiency of linear layers for the bulk of computation. For LOB order flow, could a similar hybrid unlock both the <em>long context</em> of linear models and the <em>precise event retrieval</em> of softmax attention? The optimal mixing ratio and layer placement for financial sequences is entirely unexplored.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q8" data-term="Q8 Hybrid Attention: Mixing Linear and Softmax Layers?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q9: GPU Kernel Bubbles -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q9</span> Minimising GPU Kernel Bubbles via Profiler-Guided Optimisation?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Individual GPU kernels execute in <strong>nanoseconds</strong>, yet the idle gaps between them can be orders of magnitude longer. These <strong>inter-kernel bubbles</strong> arise from host-side dispatch overhead, synchronisation barriers, small kernels that underutilise SMs, and memory allocation stalls. When kernels themselves run in tens to hundreds of nanoseconds, even a microsecond of bubble time means the GPU is idle 90%+ of the wall clock. For SSM training, where each step involves many small operations (element-wise gates, scans, reductions), the bubble fraction can easily exceed the useful compute fraction.
                    </p>
                    <p>
                        The principled approach is <strong>profiler-guided optimisation</strong>: use tools like <a href="https://docs.nvidia.com/nsight-systems/"><strong>Nsight Systems</strong></a>, <strong>XProfiler</strong>, or JAX's built-in profiler to generate GPU execution timelines, identify the largest bubbles, and systematically eliminate them through kernel fusion, async memory transfers, CUDA graph capture, or reordering operations to overlap compute with communication. For LOBS5, which chains SSM scans with MLP blocks and loss computation, how much wall-clock time is lost to bubbles, and what is the theoretical speedup from closing them?
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q9" data-term="Q9 Minimising GPU Kernel Bubbles via Profiler-Guided Optimisation?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q10: Hidden Orders -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q10</span> Modelling Hidden Orders and Dark Liquidity?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Our current model only observes <strong>visible order flow</strong>, the limit orders, cancellations, and trades that appear in the public ITCH feed. However, <strong>hidden orders</strong> (iceberg orders, reserve orders, and dark pool executions) account for approximately <strong>20% of total market liquidity</strong>. These orders are invisible to the model yet materially affect price dynamics: a large hidden buy order silently absorbs sell-side liquidity, causing the price to rise less than the visible flow would predict.
                    </p>
                    <p>
                        Ignoring hidden liquidity means our model systematically overestimates price impact and underestimates available depth. Can a foundation model learn to <em>infer</em> the presence of hidden orders from observable signatures, such as unexplained price support, unusual fill rates, or discrepancies between expected and realised impact? Alternatively, should we explicitly model hidden liquidity as a latent variable? This is both a data problem (can we obtain or reconstruct hidden order information?) and a modelling problem (what architecture captures partial observability?).
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q10" data-term="Q10 Modelling Hidden Orders and Dark Liquidity?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q11: Cancellation Modelling -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q11</span> How Should Cancellation Orders Be Modelled?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Cancellations account for a large fraction of order flow, often exceeding 90% of all messages. Yet modelling them in a foundation model poses unique <strong>vocabulary and architecture design</strong> challenges. A cancellation must reference a <em>specific prior order</em>, but how should this reference be encoded?
                    </p>
                    <p>
                        Two natural approaches exist: <strong>(1) Cancel by time</strong>, specify the relative or absolute timestamp of the order to cancel, embedding temporal distance into the vocabulary; or <strong>(2) Cancel by order ID</strong>, generate a token referencing the original order's identifier. Approach (1) avoids an exploding vocabulary but is ambiguous when multiple orders share similar timestamps. Approach (2) is precise but requires the model to maintain and index an ever-growing set of live order IDs, a form of <em>pointer network</em> or <em>copy mechanism</em> that standard autoregressive architectures do not natively support.
                    </p>
                    <p>
                        The design choice affects both the <strong>token vocabulary</strong> (fixed-size categorical vs. dynamic pointer) and the <strong>model architecture</strong> (does the model need an explicit memory of open orders?). Getting cancellations right is critical because they are the primary mechanism through which market makers manage inventory risk and adjust to new information.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q11" data-term="Q11 How Should Cancellation Orders Be Modelled?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q12: Tokenisation Trade-off -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q12</span> The Tokenisation Trade-off: Sequence Length vs. Vocabulary Size?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        For a fixed context window of $N$ orders, there is a fundamental tension between <strong>tokens per message</strong> and <strong>vocabulary size</strong>. Using more tokens per message (e.g., our 24-token encoding) preserves full fidelity but makes the sequence very long ($24N$ tokens for $N$ orders), stressing the model's context capacity. Using fewer tokens per message (down to 1-token encodings like MarS or LOBERT) keeps the sequence short ($N$ tokens) but requires a much larger vocabulary to encode the same information, increasing the embedding table size and softmax computation.
                    </p>
                    <p>
                        This is a direct trade-off: <strong>long sequences with small vocabulary</strong> vs. <strong>short sequences with large vocabulary</strong>. For a context of 500 orders, our encoding produces 12,000 tokens while a 1-token encoding produces 500 tokens but may need a vocabulary of 100k+ to avoid information loss. Which regime is more efficient depends on the architecture (SSMs handle long sequences cheaply; Transformers prefer shorter sequences but can handle large vocabularies via efficient softmax). The optimal operating point on this Pareto frontier for LOB modelling is unknown.
                    </p>
                    <p>
                        Large vocabularies also create a concrete engineering problem: the <strong>cross-entropy loss layer</strong> materialises a logit matrix of size (batch &times; seq_len &times; vocab_size), which can consume more memory than the rest of the model combined. Techniques like <a href="https://arxiv.org/abs/2411.09009"><strong>Cut Cross-Entropy</strong></a> (CCE) avoid materialising the full logit matrix by computing only the logit for the correct token and evaluating log-sum-exp on the fly. In practice, we sidestepped this by switching from a 22-token encoding (base-10,000 prices, vocab ~12k) to a <strong>24-token encoding</strong> (base-100 prices), which cuts ~9,900 entries from the vocabulary at the cost of just 2 extra tokens per message.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q12" data-term="Q12 The Tokenisation Trade-off: Sequence Length vs. Vocabulary Size?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ========== SPONSOR ========== -->
            <h2>Support This Research</h2>

            <!-- Timeline (green box) -->
            <div style="border-radius: 12px; padding: 25px 30px; margin-bottom: 20px; background-color: #f0fff4; border: 1px solid #c0f0c8;">
                <div style="position: relative; max-width: 650px; margin: 0 auto; padding: 0 10px;">
                    <!-- Donations above the line -->
                    <div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
                        <div style="text-align: center; width: 20%; display:flex; align-items:center; justify-content:center;">
                            <div style="font-size: 12px; font-weight: 700; color: #555;">Donations<br>&amp; Grants</div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700; color: #333; line-height: 1.4;">JP Morgan AI<br><span style="font-weight: 400;">8&times; L40S Server</span></div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700; color: #333; line-height: 1.4;">XTX Markets<br><span style="font-weight: 400;">32&times; B200 Server</span></div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700; color: #333; line-height: 1.4;">Isambard-AI<br><span style="font-weight: 400;">600k H100 GPU hrs</span></div>
                        </div>
                        <div style="text-align: center; width: 20%;"></div>
                    </div>
                    <!-- Arrow line -->
                    <div style="position: relative; height: 30px;">
                        <div style="position: absolute; top: 14px; left: 10px; right: 10px; height: 3px; background: #555;"></div>
                        <div style="position: absolute; top: 8px; right: 5px; width: 0; height: 0; border-top: 8px solid transparent; border-bottom: 8px solid transparent; border-left: 14px solid #555;"></div>
                        <!-- Dots -->
                        <div style="display: flex; justify-content: space-between; position: absolute; top: 0; left: 0; right: 0;">
                            <div style="width: 20%; display:flex; justify-content:center;"><div style="width: 12px; height: 12px; background: #333; border-radius: 50%; margin-top: 8px;"></div></div>
                            <div style="width: 20%; display:flex; justify-content:center;"><div style="width: 12px; height: 12px; background: #333; border-radius: 50%; margin-top: 8px;"></div></div>
                            <div style="width: 20%; display:flex; justify-content:center;"><div style="width: 12px; height: 12px; background: #333; border-radius: 50%; margin-top: 8px;"></div></div>
                            <div style="width: 20%; display:flex; justify-content:center;"><div style="width: 12px; height: 12px; background: #333; border-radius: 50%; margin-top: 8px;"></div></div>
                            <div style="width: 20%; display:flex; justify-content:center;"><div style="width: 12px; height: 12px; background: #333; border-radius: 50%; margin-top: 8px;"></div></div>
                        </div>
                    </div>
                    <!-- Years and papers below the line -->
                    <div style="display: flex; justify-content: space-between; margin-top: 6px;">
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700;">2022</div>
                            <div style="font-size: 11px; color: #666; margin-top: 2px;">Project start</div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700;">2023</div>
                            <div style="font-size: 11px; color: #666; margin-top: 2px;">LOBS5 v1<br>JAX-LOB</div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700;">2024</div>
                            <div style="font-size: 11px; color: #666; margin-top: 2px;">MoE, Scaling<br>LOB-Bench</div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700;">2025</div>
                            <div style="font-size: 11px; color: #666; margin-top: 2px;">LOBS5 v2<br>Diffusion, JaxMARL-HFT</div>
                        </div>
                        <div style="text-align: center; width: 20%;">
                            <div style="font-size: 13px; font-weight: 700;">2026</div>
                            <div style="font-size: 11px; color: #666; margin-top: 2px;">&sigma;<sub>0</sub> release</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Sponsor (blue box) -->
            <div style="border-radius: 12px; padding: 25px; background-color: #f0f4ff; border: 1px solid #c0d0f0;">
            <p style="margin-top: 0;">
                Scaling foundation models for finance requires substantial resources. If you or your organisation can help, we would be grateful for support in any of the following areas:
            </p>

            <div class="sponsor-grid">
                <!-- Compute -->
                <div style="border: 1px solid #d0d0d0; border-radius: 10px; padding: 15px 18px; background: #fafafa;">
                    <h3 style="margin: 0 0 10px; font-size: 15px;"><i class="fa fa-server" style="margin-right: 6px; color: #555;"></i>Compute</h3>
                    <table class="data-table" style="max-width: 100%; margin: 0; font-size: 13px;">
                        <tbody>
                            <tr><td style="font-weight:600; width:35%;">GPU / TPU clusters</td><td>Multi-node training (GPU or TPU)</td></tr>
                            <tr><td style="font-weight:600;">Cloud credits</td><td>AWS, GCP, Azure, or Lambda</td></tr>
                            <tr><td style="font-weight:600;">API tokens</td><td>Anthropic, OpenAI, or other LLM API credits</td></tr>
                        </tbody>
                    </table>
                </div>
                <!-- Data -->
                <div style="border: 1px solid #d0d0d0; border-radius: 10px; padding: 15px 18px; background: #fafafa;">
                    <h3 style="margin: 0 0 10px; font-size: 15px;"><i class="fa fa-database" style="margin-right: 6px; color: #555;"></i>Data</h3>
                    <table class="data-table" style="max-width: 100%; margin: 0; font-size: 13px;">
                        <tbody>
                            <tr><td style="font-weight:600; width:35%;">Options markets</td><td>Full LOB data (CBOE, CME, Eurex)</td></tr>
                            <tr><td style="font-weight:600;">International equities</td><td>Non-US markets (LSE, TSE, SSE, HKEX)</td></tr>
                            <tr><td style="font-weight:600;">Other asset classes</td><td>Futures, FX, or crypto at message level</td></tr>
                        </tbody>
                    </table>
                </div>
                <!-- Funding -->
                <div style="border: 1px solid #d0d0d0; border-radius: 10px; padding: 15px 18px; background: #fafafa;">
                    <h3 style="margin: 0 0 10px; font-size: 15px;"><i class="fa fa-graduation-cap" style="margin-right: 6px; color: #555;"></i>Funding</h3>
                    <table class="data-table" style="max-width: 100%; margin: 0; font-size: 13px;">
                        <tbody>
                            <tr><td style="font-weight:600; width:35%;">PhD studentships</td><td>Sponsorship for doctoral researchers</td></tr>
                            <tr><td style="font-weight:600;">Research grants</td><td>Collaborative grants or industry partnerships</td></tr>
                        </tbody>
                    </table>
                </div>
                <!-- Live Trading -->
                <div style="border: 1px solid #d0d0d0; border-radius: 10px; padding: 15px 18px; background: #fafafa;">
                    <h3 style="margin: 0 0 10px; font-size: 15px;"><i class="fa fa-line-chart" style="margin-right: 6px; color: #555;"></i>Live Trading</h3>
                    <table class="data-table" style="max-width: 100%; margin: 0; font-size: 13px;">
                        <tbody>
                            <tr><td style="font-weight:600; width:35%;">Model deployment</td><td>Deploy LOBS5 to live trading infrastructure</td></tr>
                            <tr><td style="font-weight:600;">Strategy backtesting</td><td>Simulation &rarr; paper trading &rarr; live execution</td></tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div style="margin-top: 20px; padding: 14px 24px; border-radius: 10px; border: 1px solid #d0d0d0; background: #fafafa; font-size: 15px; text-align: center;">
                Interested? Reach out to <strong>Jakob Foerster</strong> at
                <a href="mailto:jakob.foerster@eng.ox.ac.uk">jakob.foerster@eng.ox.ac.uk</a>
            </div>
            </div>

            <div style="margin-top: 25px;"></div>

            <!-- ========== CONTRIBUTE ========== -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <i class="fa fa-users" style="margin-right: 8px;"></i> Contribute
                </button>
                <div class="oq-body" style="display:block;">
                    <p>
                        Beyond funding and data, we are looking for collaborators who want to contribute directly to the project:
                    </p>
                    <table class="data-table" style="max-width: 100%;">
                        <thead>
                            <tr><th style="width: 30%;"><i class="fa fa-video-camera"></i>&nbsp; Media</th><th>What we need</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Graphics &amp; GIFs</td><td>Design explanatory diagrams, animated GIFs, and infographics illustrating model architecture, order book mechanics, and experimental results</td></tr>
                            <tr><td>Video production</td><td>Create explainer videos for YouTube or TikTok: demos, visualisations of order book dynamics and model behaviour</td></tr>
                        </tbody>
                    </table>
                    <table class="data-table" style="max-width: 100%; margin-top: 15px;">
                        <thead>
                            <tr><th style="width: 30%;"><i class="fa fa-plug"></i>&nbsp; Infrastructure</th><th>What we need</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Trading server setup</td><td>Set up servers that simulate real exchange connectivity, including realistic submission latency, network routing, and order queue position effects</td></tr>
                            <tr><td>Low-latency networking</td><td>Optimise the order submission pipeline: serialisation, router hops, and co-location simulation for realistic end-to-end latency profiling</td></tr>
                            <tr><td>C/C++ reimplement</td><td>Rewrite inference and simulator components in C/C++ for production-grade speed in live trading, replacing current JAX/Python implementations where latency is critical</td></tr>
                            <tr><td>Google TPU support</td><td>Port training and inference pipelines to TPU (v4/v5e), including JAX TPU-specific optimizations, multi-pod scaling, and TPU-compatible custom kernels</td></tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- ========== HANDS-ON ========== -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <i class="fa fa-wrench" style="margin-right: 8px;"></i> Hands-on the Project
                </button>
                <div class="oq-body" style="display:block;">
                    <p>
                        Can't solve the Open Questions yet, but want to get involved? We offer hands-on engineering and research experience in the following areas:
                    </p>
                    <table class="data-table" style="max-width: 100%;">
                        <thead>
                            <tr><th style="width: 35%;">Skill</th><th>What you'll work on</th></tr>
                        </thead>
                        <tbody>
                            <tr><td><strong>Mixture of Experts</strong></td><td>Native MoE training, expert routing, load balancing, and capacity factors at scale</td></tr>
                            <tr><td><strong>Mixed Precision</strong></td><td>FP16/BF16/FP8 training pipelines, loss scaling, and numerical stability for SSMs</td></tr>
                            <tr><td><strong>Triton Kernels</strong></td><td>Write custom GPU kernels in Triton for order book operations and model inference</td></tr>
                            <tr><td><strong>Kernel Fusion</strong></td><td>Fuse multi-step operations (scan + matmul + activation) into single kernel launches to minimise HBM round-trips</td></tr>
                            <tr><td><strong>Attention Modules</strong></td><td>Implement and benchmark attention variants (FlashAttention, linear attention, sliding window) for financial sequences</td></tr>
                            <tr><td><strong>Agentic RL</strong></td><td>Reinforcement learning for trading tasks (order execution, market making, statistical arbitrage): reward shaping, policy gradient methods, and environment integration with the simulator stack</td></tr>
                            <tr><td><strong>Flow Matching Post-training</strong></td><td>Discrete Flow Matching to convert pretrained autoregressive models into bidirectional generators with reduced compound error</td></tr>
                        </tbody>
                    </table>
                    <div style="margin-top: 20px; padding: 18px 24px; border-radius: 10px; border: 1px solid #d0d0d0; background: #fafafa; font-size: 15px; text-align: center; line-height: 1.8;">
                        If you want to work on large-scale <span style="color: #e74c3c;">structured time series</span> foundation models,<br>training on <span style="color: #e74c3c;">trillions of tokens</span> across <span style="color: #e74c3c;">thousands of H100 GPUs</span>,<br>this is the project for you.
                    </div>
                    <div style="margin-top: 15px; padding: 14px 24px; border-radius: 10px; border: 1px solid #d0d0d0; background: #fafafa; font-size: 15px; text-align: center;">
                        Contact: <a href="mailto:kang@robots.ox.ac.uk">kang@robots.ox.ac.uk</a> or <a href="mailto:jakob@robots.ox.ac.uk">jakob@robots.ox.ac.uk</a>
                    </div>
                </div>
            </div>

            <!-- ========== CITATION ========== -->
            <h2>Citation</h2>
            <div style="margin-top: 15px;">
                <textarea id="bibtex" style="display:none;">
@misc{lobs5_2025,
    title={Scaling Laws for Quantitative Finance Foundation Models},
    author={Ani Calinescu and Bidipta Sarkar and Reuben and Jakob Foerster and Kang Li and Mihai Cucuringu and Satyam and Gereon Franken and Peer Nagy and Stefan Zohren and Aramis Marti-Shahandeh and George Nigmatulin and Valentin and Sascha Frey and Frensi Zejnullahu and Alfie Backhouse},
    year={2025}
}
                </textarea>
            </div>

            <!-- ========== FOOTER ========== -->
            <div class="text-center" style="margin-top: 20px; margin-bottom: 30px; font-size: 12px; color: #999;">
                <p>
                    Template adapted from <a href="https://github.com/EasyAcademicWebsite/EasyAcademicWebsite.github.io">EasyAcademicWebsite</a>.
                </p>
            </div>

        </div><!-- col -->
    </div><!-- row -->
</div><!-- container -->

<!-- Scripts -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
<script src="utils/app.js"></script>

<!-- Dynamic author shuffle with seeded PRNG -->
<script>
(function() {
    // Seeded PRNG (mulberry32)
    function mulberry32(seed) {
        return function() {
            seed |= 0; seed = seed + 0x6D2B79F5 | 0;
            var t = Math.imul(seed ^ seed >>> 15, 1 | seed);
            t = t + Math.imul(t ^ t >>> 7, 61 | t) ^ t;
            return ((t ^ t >>> 14) >>> 0) / 4294967296;
        };
    }

    // Fisher-Yates shuffle with seeded random
    function shuffle(arr, rng) {
        var a = arr.slice();
        for (var i = a.length - 1; i > 0; i--) {
            var j = Math.floor(rng() * (i + 1));
            var tmp = a[i]; a[i] = a[j]; a[j] = tmp;
        }
        return a;
    }

    var faculty = new Set([
        'Stefan Zohren', 'Ani Calinescu', 'Jakob Foerster', 'Mihai Cucuringu',
        'Leandro S\u00e1nchez-Betancourt'
    ]);

    var allAuthors = [
        'Ani Calinescu', 'Bidipta Sarkar', 'Jakob Foerster',
        'Kang Li', 'Mihai Cucuringu', 'Satyam',
        'Stefan Zohren', 'Aramis Marti-Shahandeh',
        'George Nigmatulin', 'Valentin', 'Sascha Frey',
        'Leandro S\u00e1nchez-Betancourt'
    ];

    var now = new Date();
    var hhmmss = now.getHours() * 10000 + now.getMinutes() * 100 + now.getSeconds();
    var rng = mulberry32(hhmmss);

    // Split into students and faculty
    var students = allAuthors.filter(function(n) { return !faculty.has(n); });
    var facultyArr = allAuthors.filter(function(n) { return faculty.has(n); });

    // Shuffle students and faculty separately
    var shuffledStudents = shuffle(students, rng);
    var rng2 = mulberry32(hhmmss + 1);
    var shuffledFaculty = shuffle(facultyArr, rng2);

    // Render student list with wide spacing (like eshyperscale)
    document.getElementById('author-list').innerHTML =
        shuffledStudents.map(function(n) {
            return '<span style="display:inline-block; margin: 0 12px;">' + n + '</span>';
        }).join('');

    // Render faculty list, mark Jakob Foerster as PI
    document.getElementById('faculty-list').innerHTML =
        '<span style="display:inline-block; margin: 0 12px;"><em>Faculty:</em></span>' + shuffledFaculty.map(function(n) {
            return '<span style="display:inline-block; margin: 0 12px;">' + n + '</span>';
        }).join('');

    // Seed note right below authors
    var pad = function(n) { return n < 10 ? '0' + n : '' + n; };
    var timeStr = pad(now.getHours()) + ':' + pad(now.getMinutes()) + ':' + pad(now.getSeconds());
    document.getElementById('seed-note').innerHTML =
        '<sup>&dagger;</sup> Author order randomized with <code>seed(' + hhmmss + ')</code>: ' +
        'current timestamp HHMMSS (' + timeStr + '). Reload to re-shuffle.';
})();
</script>

<!-- Open Questions toggle -->
<script>
function toggleOQ(btn) {
    var body = btn.nextElementSibling;
    if (!body) return;
    body.style.display = body.style.display === 'none' ? 'block' : 'none';
}

// Nav buttons: mutually exclusive (Papers, Code, Checkpoint, Past Contributors)
function toggleNav(id) {
    var el = document.getElementById(id);
    if (!el) return;
    var isOpen = el.style.display === 'block';
    // Close all nav dropdowns
    document.querySelectorAll('.nav-dropdown').forEach(function(d) { d.style.display = 'none'; });
    // Toggle clicked one
    if (!isOpen) el.style.display = 'block';
}

// Single shared giscus widget - moved between questions
var giscusLoaded = false;
var giscusContainer = document.createElement('div');
giscusContainer.id = 'shared-giscus';
giscusContainer.style.marginTop = '12px';

function loadComments(btn) {
    var container = btn.nextElementSibling;
    if (!container) return;

    // If clicking same question again, toggle off
    if (container.contains(giscusContainer) && container.style.display === 'block') {
        container.style.display = 'none';
        return;
    }

    // Hide any previously open comment containers
    document.querySelectorAll('.oq-comment-toggle .oq-body').forEach(function(el) {
        el.style.display = 'none';
    });

    container.style.display = 'block';
    var gDiv = container.querySelector('[data-term]');
    if (!gDiv) return;
    var term = gDiv.dataset.term;

    // Move shared container into this question
    gDiv.innerHTML = '';
    gDiv.appendChild(giscusContainer);

    if (!giscusLoaded) {
        // First load: create the giscus widget
        var script = document.createElement('script');
        script.src = 'https://giscus.app/client.js';
        script.setAttribute('data-repo', 'KangOxford/sigma0');
        script.setAttribute('data-repo-id', 'R_kgDORKDiSQ');
        script.setAttribute('data-category', 'General');
        script.setAttribute('data-category-id', 'DIC_kwDORKDiSc4C1-T6');
        script.setAttribute('data-mapping', 'specific');
        script.setAttribute('data-term', term);
        script.setAttribute('data-strict', '0');
        script.setAttribute('data-reactions-enabled', '1');
        script.setAttribute('data-emit-metadata', '0');
        script.setAttribute('data-input-position', 'bottom');
        script.setAttribute('data-theme', 'light');
        script.setAttribute('data-lang', 'en');
        script.setAttribute('crossorigin', 'anonymous');
        script.async = true;
        giscusContainer.appendChild(script);
        giscusLoaded = true;
    } else {
        // Already loaded: use postMessage to switch Discussion
        var iframe = document.querySelector('iframe.giscus-frame');
        if (iframe) {
            iframe.contentWindow.postMessage(
                { giscus: { setConfig: { term: term } } },
                'https://giscus.app'
            );
        }
    }
}

// Watch for giscus iframe insertion and scale it down
var giscusObserver = new MutationObserver(function() {
    var iframe = document.querySelector('#shared-giscus iframe.giscus-frame');
    if (iframe && !iframe.dataset.scaled) {
        iframe.style.transform = 'scale(0.75)';
        iframe.style.transformOrigin = 'top left';
        iframe.style.width = '133%';
        iframe.dataset.scaled = '1';
    }
});
giscusObserver.observe(giscusContainer, { childList: true, subtree: true });
</script>

<!-- Collapsible toggle -->
<script>
document.querySelectorAll('.collapsible').forEach(function(btn) {
    btn.addEventListener('click', function() {
        this.classList.toggle('active');
        var content = this.nextElementSibling;
        content.classList.toggle('show');
    });
});
</script>

</body>
</html>
