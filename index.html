<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Scaling Laws for Quantitative Finance Foundation Models - LOBS5">
    <meta name="keywords" content="foundation model, limit order book, state space model, order flow, quantitative finance, scaling laws">
    <title>LOBS5 - Scaling Laws for Quantitative Finance Foundation Models</title>
    <link rel="icon" type="image/png" href="imgs/favicon.png">
    <link rel="apple-touch-icon" href="imgs/favicon.png">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- CodeMirror (for BibTeX display) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <!-- Template styles -->
    <link rel="stylesheet" href="utils/style.css">
    <!-- Custom styles -->
    <link rel="stylesheet" href="style.css">

    <!-- MathJax 3 -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">
        <div class="col-md-10 col-md-offset-1">

            <!-- ========== TITLE ========== -->
            <div class="text-center" style="margin-top: 40px;">
                <h1 style="font-size: 48px; font-weight: 700; font-family: 'Georgia', 'Times New Roman', serif;">
                    <em>&sigma;<sub>0</sub></em>: Unlock the Large Scale Quant Foundation Models
                </h1>
            </div>

            <!-- ========== AUTHORS (dynamically shuffled) ========== -->
            <div class="text-center" style="margin-top: 20px;">
                <div id="author-list" style="font-size: 15px; line-height: 2.2em;"></div>
                <div id="faculty-list" style="font-size: 13px; color: #777; margin-top: 8px;"></div>
                <p id="seed-note" style="font-size: 12px; color: #999; margin-top: 8px;"></p>
                <p style="font-size: 13px; color: #555; margin-top: 6px;">
                    Contact: <a href="mailto:kang@robots.ox.ac.uk">kang@robots.ox.ac.uk</a> or <a href="mailto:jakob@robots.ox.ac.uk">jakob@robots.ox.ac.uk</a>
                </p>
            </div>

            <!-- ========== AFFILIATIONS (logos) ========== -->
            <div class="text-center" style="margin-top: 15px; margin-bottom: 35px;">
                <img src="imgs/oxford_logo.png" alt="University of Oxford" height="48" style="display: inline-block; margin: 0 20px; vertical-align: middle;">
                <img src="imgs/flair_logo.png" alt="FLAIR" height="48" style="display: inline-block; margin: 0 20px; vertical-align: middle;">
            </div>

            <!-- ========== NAV BUTTONS (horizontal row) ========== -->
            <div class="text-center" style="margin-top: 20px; margin-bottom: 30px;">
                <button onclick="var el=document.getElementById('papers-list');el.style.display=el.style.display==='none'?'block':'none';" style="display: inline-block; margin: 0 8px; padding: 10px 24px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; background: #fafafa; cursor: pointer;">
                    <i class="fa fa-file-text-o" style="margin-right: 6px;"></i>Papers
                </button>
                <a href="#" style="display: inline-block; margin: 0 8px; padding: 10px 24px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; text-decoration: none; background: #fafafa;">
                    <i class="fa fa-github" style="margin-right: 6px;"></i>Code
                </a>
                <a href="#" style="display: inline-block; margin: 0 8px; padding: 10px 24px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; text-decoration: none; background: #fafafa;">
                    <i class="fa fa-database" style="margin-right: 6px;"></i>Checkpoint
                </a>
                <button onclick="var el=document.getElementById('past-contrib-list');el.style.display=el.style.display==='none'?'block':'none';" style="display: inline-block; margin: 0 8px; padding: 10px 24px; border: 1px solid #d0d0d0; border-radius: 8px; color: #333; font-size: 14px; background: #fafafa; cursor: pointer;">
                    <i class="fa fa-users" style="margin-right: 6px;"></i>Past Contributors
                </button>

                <!-- Papers dropdown -->
                <div id="papers-list" style="display: none; margin-top: 15px; text-align: left; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px 20px; background: #fafafa;">
                    <table style="width: 100%; border: none;">
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2309.00638" style="font-size: 14px; font-weight: 600;">Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network</a><br>
                                <span style="font-size: 12px; color: #666;">P Nagy, S Frey, S Sapora, K Li, A Calinescu, S Zohren, J Foerster</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Oral] ICAIF 2023</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 34</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2308.13289" style="font-size: 14px; font-weight: 600;">JAX-LOB: A GPU-Accelerated Limit Order Book Simulator to Unlock Large Scale Reinforcement Learning for Trading</a><br>
                                <span style="font-size: 12px; color: #666;">SY Frey, K Li, P Nagy, S Sapora, C Lu, S Zohren, J Foerster, A Calinescu</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Best Paper] ICAIF 2023</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 34</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2502.09172" style="font-size: 14px; font-weight: 600;">LOB-Bench: Benchmarking Generative AI for Finance &mdash; an Application to Limit Order Book Data</a><br>
                                <span style="font-size: 12px; color: #666;">P Nagy, S Frey, K Li, B Sarkar, S Vyetrenko, S Zohren, A Calinescu, ...</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">ICML 2025</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 5</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://dl.acm.org/doi/10.1145/3677052.3698691" style="font-size: 14px; font-weight: 600;">Mixtures of Experts for Scaling Up Neural Networks in Order Execution</a><br>
                                <span style="font-size: 12px; color: #666;">K Li, M Cucuringu, L S&aacute;nchez-Betancourt, T Willi</span><br>
                                <span style="font-size: 12px; color: #999;">ICAIF 2024 &nbsp;&middot;&nbsp; Cited: 5</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://dl.acm.org/doi/10.1145/3768292.3768314" style="font-size: 14px; font-weight: 600;">Discrete Flow Matching is a Surprisingly Effective Post-training Method to Address Compound Error in Autoregressive Models</a><br>
                                <span style="font-size: 12px; color: #666;">K Li, B Sarkar, Z Xiong, S Frey, Z Wang, F Zejnullahu, A Backhouse, ...</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Oral] ICAIF 2025</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 1</span>
                            </td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://dl.acm.org/doi/pdf/10.1145/3768292.3770416" style="font-size: 14px; font-weight: 600;">JaxMARL-HFT: GPU-Accelerated Large-Scale Multi-Agent Reinforcement Learning for High-Frequency Trading</a><br>
                                <span style="font-size: 12px; color: #666;">V Mohl, S Frey, R Leyland, K Li, G Nigmatulin, M Cucuringu, S Zohren, ...</span><br>
                                <span style="font-size: 12px; color: #e74c3c; font-weight: 600;">[Best Paper] ICAIF 2025</span> <span style="font-size: 12px; color: #999;">&nbsp;&middot;&nbsp; Cited: 1</span>
                            </td>
                        </tr>
                        <tr>
                            <td style="padding: 10px 5px; border: none; vertical-align: top;">
                                <a href="https://arxiv.org/abs/2509.05107" style="font-size: 14px; font-weight: 600;">Painting the Market: Generative Diffusion Models for Financial Limit Order Book Simulation and Forecasting</a><br>
                                <span style="font-size: 12px; color: #666;">A Backhouse, K Li, J Foerster, A Calinescu, S Zohren</span><br>
                                <span style="font-size: 12px; color: #999;">arXiv 2025</span>
                            </td>
                        </tr>
                    </table>
                </div>

                <!-- Past contributors dropdown -->
                <div id="past-contrib-list" style="display: none; margin-top: 10px;">
                    <p style="font-size: 13px; color: #777; text-align: center; margin: 8px 0;">
                        Peer Nagy &middot; Alfie Backhouse &middot; Reuben &middot; Frensi Zejnullahu &middot; Chris Lu &middot; Gereon Franken &middot; Silvia Sapora &middot; Timon Willi
                    </p>
                </div>
            </div>

            <!-- ========== HERO IMAGE ========== -->
            <div id="header_img">
                <img src="imgs/scaling_law.png" alt="Scaling Laws for LOB Foundation Models" style="width: 90%;">
                <p class="fig-caption">
                    <strong>Figure 1.</strong> Scaling laws for LOBS5: test loss decreases monotonically with model size (34M to 617M parameters), trained on 25 billion tokens of NASDAQ order flow. The ratio indicates tokens-per-parameter.
                </p>
            </div>

            <!-- ========== TL;DR ========== -->
            <blockquote>
                <strong>TL;DR</strong> &mdash; Finance is one of the hardest domains for ML &mdash; noisy, fast, strategic. We develop <strong>LOBS5</strong>, the first state-space foundation model for limit order book data, trained on <strong>25 billion tokens</strong> of NASDAQ order flow. Our three-stage pipeline &mdash; pre-training, post-training with Discrete Flow Matching, and fine-tuning with Evolution Strategies &mdash; achieves <strong>155% PnL improvement</strong> on order execution and exhibits clear scaling laws from 34M to 2B+ parameters.
            </blockquote>

            <!-- ========== ABSTRACT ========== -->
            <h2>Abstract</h2>
            <blockquote>
            <p>
                Financial markets operate through limit order books (LOBs), which aggregate market participants' trading intentions in the form of limit orders specifying order type, direction, price, and quantity. The continuous stream of these orders&mdash;the <em>order flow</em>&mdash;provides a high-fidelity record of market dynamics at unprecedented scale: the S&amp;P 500 constituents alone generated approximately <strong>3.8 trillion tokens</strong> of order flow between 2016 and 2021, comparable to the largest natural language corpora.
            </p>
            <p>
                We introduce <strong>LOBS5</strong>, a foundation model for LOB data built on the S5 State Space Model architecture, pretrained on 25 billion tokens from NASDAQ using a 24-token encoding scheme that discretizes each limit order message into categorical tokens. Unlike natural language where tokens are purely symbolic, order flow comprises both categorical values (event type, direction) and numerical values (price, quantity) in which magnitude carries semantic meaning&mdash;making LOB order flow an ideal testbed for foundation models on structured time series.
            </p>
            <p>
                Our complete training pipeline consists of three stages: (1) S5-based pre-training with <em>Sliding Window Recurrences</em> for long context processing, (2) <em>Discrete Flow Matching</em> post-training that eliminates compound error through bidirectional refinement (48.6% reduction in Wasserstein distance), and (3) <em>Evolution Strategies</em> fine-tuning via EGGROLL that achieves 155% PnL improvement on order execution tasks with 65,536 parallel simulations. Pre-training exhibits clear scaling laws: test loss decreases monotonically from 34M to 617M parameters, achieving approximately 40% Model FLOPs Utilization comparable to Meta's Llama-3.1.
            </p>
            </blockquote>

            <!-- ========== WHY ORDER FLOW ========== -->
            <h2>Why Order Flow?</h2>
            <p>
                The stock exchanges (NYSE and NASDAQ) generate massive volumes of high-frequency trading data, with new orders created at the millisecond level. These orders encapsulate a wealth of insights: the tendencies and preferences of various market participants, underlying market information, and the correlations between different stocks.
            </p>
            <p>
                Our core objective is to train a foundation model on this terabyte-scale dataset, enabling it to identify the transition dynamics of limit order books. Through post-training, we fine-tune this large model for downstream tasks such as prediction, classification, and order execution. Even a few basis points of improvement in execution can translate to enormous profits when deployed at scale.
            </p>

            <button class="collapsible">The Multi-Agent Perspective</button>
            <div class="contentx">
                <p style="margin-top: 12px;">
                    At its core, a limit order book is an <strong>anonymous multi-agent system</strong>. Each market participant submits orders (actions) that modify the shared order book (state). The <em>state transitions</em> are governed by the matching engine (deterministic), but the <em>distribution of future states</em> depends on the collective behavior of all participants&mdash;analogous to video frame prediction in computer vision.
                </p>
                <p>
                    LOBS5 captures this state distribution through next-token prediction on order flow tokens. The foundation model learns two fundamental capabilities:
                </p>
                <ul>
                    <li><strong>Mimic historical orders</strong>: Generate realistic background market activity as a world model</li>
                    <li><strong>Optimize trading policy</strong>: Fine-tune an ego agent for downstream tasks using evolution strategies</li>
                </ul>
                <p>
                    This enables simulation of <em>dynamic price impact</em> (not static), <em>indirect price impact</em> (not direct), and <em>adversarial selection</em>&mdash;where other agents observe and react to the ego agent's behavior, changing their original plans. These phenomena are critical for realistic market simulation but are absent from traditional backtesting frameworks.
                </p>
            </div>

            <!-- ========== PRE-TRAINING ========== -->
            <h2>LOBS5: Pre-training at Scale</h2>

            <h3>Architecture: S5 State Space Model</h3>
            <p>
                LOBS5 is built on the <strong>S5 State Space Model</strong>, a modern alternative to Transformers that provides linear-time $O(n)$ complexity for sequence modeling, compared to the quadratic $O(n^2)$ cost of self-attention. The choice of S5 is motivated by three factors:
            </p>
            <ol>
                <li><strong>Linear complexity</strong>: Efficient processing of long order flow sequences (10,000+ tokens)</li>
                <li><strong>Natural bridge</strong>: The state space formulation naturally connects continuous market dynamics with discrete observations</li>
                <li><strong>Memory efficiency</strong>: 40&ndash;60% lower memory usage compared to Transformers</li>
            </ol>
            <p>
                A key open question is whether LOB modeling requires the full quadratic attention mechanism, or whether linear attention with longer context windows is sufficient. Our scaling results suggest the latter&mdash;LOBS5 achieves strong performance with linear complexity, and performance continues to improve with model scale.
            </p>

            <h3>Long Context: What Does 10k Orders Mean?</h3>
            <p>
                With a TBPTT window of 20 segments (each 512 tokens), LOBS5 processes approximately <strong>10,000 orders</strong> per training context. But how much real market time does that represent? The answer depends heavily on market activity:
            </p>
            <img src="imgs/context_window_time.png" alt="10k messages wall-clock time across years and activity levels" style="width: 85%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 3.</strong> Wall-clock time spanned by 10k order messages (GOOG, 2016&ndash;2023). High-activity periods (red) compress 10k orders into just 2&ndash;5 minutes, while low-activity periods (gray) can stretch to 30+ minutes. The secular trend toward higher activity means recent data packs more information per context window.
            </p>

            <h3>24-Token Encoding</h3>
            <p>
                Each limit order message is discretized into <strong>24 categorical tokens</strong> representing:
            </p>
            <ul>
                <li><strong>Event type</strong>: new order, cancellation, execution, etc.</li>
                <li><strong>Direction</strong>: buy or sell</li>
                <li><strong>Relative price levels</strong>: distance from best bid/ask</li>
                <li><strong>Size digits</strong>: order quantity decomposed into individual digits</li>
            </ul>
            <p>
                This encoding enables the application of language modeling techniques (next-token prediction with cross-entropy loss) to financial time series while preserving the semantic meaning of numerical values.
            </p>

            <h3>Scaling Laws</h3>
            <p>
                Pre-training on 25 billion tokens of Google (GOOG) stock order flow from 2022 reveals clear power-law scaling:
            </p>

            <img src="imgs/scaling_law.png" alt="Loss vs Model Parameters" style="width: 80%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 2.</strong> Test loss vs. model parameters (25B tokens). Loss decreases monotonically from 1.35 (34M) to 0.90 (617M), following a power-law relationship. Ratio = tokens / parameters.
            </p>

            <h3>GPU Efficiency</h3>
            <p>
                We achieve approximately <strong>40% Model FLOPs Utilization</strong> (MFU) during pre-training, comparable to Meta's Llama-3.1 (38&ndash;43%):
            </p>

            <table class="data-table">
                <thead>
                    <tr>
                        <th>Model Size</th>
                        <th>MFU</th>
                        <th>GPU Util %</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>2.17B</td><td>38.4%</td><td>93.4 &plusmn; 7.9</td></tr>
                    <tr><td>1.95B</td><td>35.3%</td><td>95.6 &plusmn; 7.4</td></tr>
                    <tr><td>1.86B</td><td>36.3%</td><td>94.2 &plusmn; 7.3</td></tr>
                    <tr><td>1.4B</td><td>41.7%</td><td>80.7 &plusmn; 37.5</td></tr>
                    <tr><td>1.0B</td><td>41.3%</td><td>72.9 &plusmn; 41.3</td></tr>
                    <tr><td>360M</td><td>36.2%</td><td>85.0 &plusmn; 33.9</td></tr>
                </tbody>
            </table>
            <p style="font-size: 13px; color: #666; text-align: center;">
                All metrics measured on a single node (4&times; GH200 GPUs).
            </p>

            <h3>Throughput</h3>
            <img src="imgs/throughput.png" alt="Message Generation Throughput" style="width: 85%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 4.</strong> Message generation throughput comparison. LOBSv2 achieves <strong>100&ndash;600&times;</strong> throughput improvement over v1, reaching up to 15,000 messages/second.
            </p>

            <h3>Generation Quality Across Versions</h3>
            <p>
                Successive iterations of LOBS5 show consistent improvement in generation fidelity. Evaluated on both INTC and GOOG order flow, LOBS5v2 (conditional and unconditional) substantially outperforms v1 across L1 and Wasserstein distance metrics&mdash;with lower values indicating closer distributional match to real market data:
            </p>
            <img src="imgs/lobs5_versions.png" alt="LOBS5 version comparison: L1 and Wasserstein distances" style="width: 85%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 4.</strong> L1 and Wasserstein distance across LOBS5 versions (v1, v2-conditional, v2-unconditional) on INTC and GOOG. Each version progressively reduces distributional distance, with v2 variants achieving roughly <strong>2&times;</strong> improvement over v1.
            </p>

            <!-- ========== POST-TRAINING ========== -->
            <h2>Post-training: Discrete Flow Matching</h2>
            <p>
                A fundamental limitation of autoregressive (AR) models is <strong>compound error accumulation</strong>: prediction errors propagate and amplify throughout sequence generation, as each token conditions on potentially incorrect previous outputs. This problem is particularly severe for financial order flow, where distributional shifts compound rapidly over long generation horizons.
            </p>
            <p>
                To address this, we introduce a post-training stage using <strong>Discrete Flow Matching (DFM)</strong>, which converts our pretrained AR model into a flow-based generator. Unlike traditional flow matching that starts from random noise, our approach leverages the pretrained model's learned distribution as the starting point, significantly reducing the learning burden.
            </p>
            <p>
                The results are striking: while AR models exhibit Wasserstein distance increasing from 3 to 20+ throughout sequence generation, DFM maintains a nearly flat error profile (3&ndash;4) across all time steps, achieving a <strong class="metric-highlight">48.6% reduction</strong> in Wasserstein distance. This post-training framework preserves the model's learned representations while eliminating the compound error inherent in left-to-right generation.
            </p>

            <h3>Market Impact: Square Root Law</h3>
            <p>
                A critical test of any order flow model is whether it reproduces the <strong>square root law of market impact</strong>&mdash;a universal empirical regularity where price response to order flow scales as $R_\pi \sim l^{1/2}$ with event lag $l$. We compare microscopic response functions between real GOOG data and three LOBS5 variants:
            </p>
            <img src="imgs/market_impact.png" alt="Microscopic response functions: Real Data vs LOBS5 variants" style="width: 95%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 5.</strong> Tick-normalised microscopic response functions for GOOG. Market orders (MO, red) exhibit the characteristic concave square-root shape, limit orders (LO, green) show negative impact, and cancellations (CA, blue) are near-zero. LOBS5v2 variants closely reproduce the real data patterns, while v1 (s5_main) shows deviations at longer lags.
            </p>

            <!-- ========== ENVIRONMENT ========== -->
            <h2>Environment: Hardware-Accelerated Order Book Simulator</h2>
            <p>
                Training and evaluating trading agents requires a fast, faithful market simulator. We build on two award-winning simulators from our group:
            </p>
            <ul>
                <li><a href="https://arxiv.org/abs/2308.13289"><strong>JAX-LOB</strong></a> &mdash; A fully vectorized limit order book simulator in JAX that implements the full NASDAQ ITCH protocol with price-time priority matching (<em>ICAIF 2023 Best Paper</em>)</li>
                <li><a href="https://dl.acm.org/doi/pdf/10.1145/3768292.3770416"><strong>JaxMARL-HFT</strong></a> &mdash; A multi-agent reinforcement learning extension that enables multiple trading agents to interact within the same order book environment (<em>ICAIF 2025 Best Paper</em>)</li>
            </ul>
            <p>
                These simulators leverage JAX's <code>vmap</code> to run <strong>65,536 parallel environments</strong> on a single GPU cluster (2,048 per GPU &times; 32 GPUs), enabling the massive rollout throughput required by Evolution Strategies.
            </p>

            <h3>The Sequential Bottleneck</h3>
            <p>
                Order book matching is inherently <strong>sequential</strong>: each trade modifies the book state, and the next match depends on the updated state. In JAX, this is implemented via <code>lax.while_loop</code>, which requires a <strong>device-to-host (D2H) synchronization</strong> on every iteration to evaluate the loop condition on CPU. This costs 110&ndash;500ms per matching operation&mdash;despite the entire order book fitting in just 4.8 KB of SRAM.
            </p>
            <p>
                The bottleneck is not compute, but <strong>memory access patterns</strong>. The data is small enough to live entirely in GPU shared memory, yet the JAX control flow forces it through the slow HBM &rarr; CPU &rarr; HBM path on every iteration.
            </p>

            <h3>Triton Kernel Fusion</h3>
            <p>
                We rewrite the order book matching engine (<code>_match_against_bid/ask_orders</code>) as fused <strong>Triton kernels</strong> that keep the entire while loop on-GPU, eliminating all D2H round-trips:
            </p>
            <ul>
                <li><strong>Zero D2H overhead</strong>: Loop condition evaluated in SRAM, no CPU synchronization needed</li>
                <li><strong>99% HBM efficiency</strong>: Order book data (100 orders &times; 6 columns &times; 4 bytes = 2.4 KB per side) pinned in shared memory for the entire kernel duration</li>
                <li><strong>Fused operations</strong>: Price-time priority search (parallel reduction), quantity deduction, trade recording, and empty-slot cleanup&mdash;all in a single kernel launch</li>
                <li><strong>Sequential correctness</strong>: The kernel preserves exact price-time priority semantics&mdash;matching is done in a for-loop within the kernel, respecting state dependencies between consecutive trades</li>
            </ul>

            <!-- ========== FINE-TUNING ========== -->
            <h2>Fine-tuning for Trading</h2>
            <p>
                Foundation models must adapt to domain-specific tasks. We demonstrate LOBS5's versatility through fine-tuning on <strong>order execution</strong>&mdash;a central problem in high-frequency trading: executing a specified quantity (e.g., sell 30 shares) within a time window while maximizing profit. This task presents a non-differentiable reward signal (realized PnL), making it unsuitable for gradient-based optimization.
            </p>

            <h3>EGGROLL + LoRA</h3>
            <p>
                We employ <strong>Evolution Strategies</strong> via EGGROLL (Evolution Guided GeneRal Optimisation via Low-rank Learning), combined with LoRA rank-4 on all projection matrices while freezing SSM parameters. EGGROLL uses low-rank perturbations $AB^\top$ where $r \ll \min(m,n)$, reducing memory from $mn$ to $r(m+n)$ per layer and achieving up to a <strong>hundredfold increase</strong> in training throughput for billion-parameter models.
            </p>

            <h3>Dual-Role Architecture</h3>
            <p>
                Critically, our pretrained foundation model serves <strong>dual roles</strong>:
            </p>
            <ul>
                <li><strong>Ego agent</strong>: The fine-tuned LOBS5 executes trades to maximize PnL</li>
                <li><strong>World model</strong>: The frozen pretrained LOBS5 generates background market activity, simulating realistic responses from other market participants</li>
            </ul>
            <p>
                This multi-agent setup enables accurate modeling of dynamic price impact, capturing how the ego agent's orders influence market microstructure through the reactions of synthetic background agents.
            </p>

            <h3>Order Injection and Price Impact</h3>
            <p>
                To validate that LOBS5 captures <strong>dynamic price impact</strong>, we inject buy or sell market orders at the junction point (step 500) and observe the midprice response. The model correctly learns that buy pressure drives prices up and sell pressure drives prices down&mdash;with the combined signal showing a persistent positive drift after injection:
            </p>
            <img src="imgs/injection_orders.png" alt="Midprice return after order injection: BUY vs SELL" style="width: 90%; margin: 20px auto;">
            <p class="fig-caption">
                <strong>Figure 7.</strong> Aggregated midprice return (mean &plusmn; 1 std) after injecting BUY (blue) and SELL*(&minus;1) (red) orders at the junction. The combined mean (purple) shows a clear positive drift post-injection, confirming that LOBS5 models realistic price impact dynamics rather than merely memorizing patterns.
            </p>

            <h3>Results</h3>
            <p>
                Fine-tuning on Google (GOOG) order flow from January 2023:
            </p>
            <table class="data-table" style="max-width: 500px; margin: 0 auto;">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Mean PnL (pretrained)</td><td>4,700</td></tr>
                    <tr><td>Mean PnL (fine-tuned)</td><td><strong>12,000</strong></td></tr>
                    <tr><td>Improvement</td><td class="metric-highlight">155%</td></tr>
                    <tr><td>Parallel environments</td><td>65,536</td></tr>
                    <tr><td>Per-GPU parallelism</td><td>2,048</td></tr>
                    <tr><td>Total GPUs</td><td>32</td></tr>
                    <tr><td>HBM access efficiency</td><td>99%</td></tr>
                </tbody>
            </table>
            <p style="margin-top: 12px;">
                The matching engine simulator uses custom CUDA kernels that parallelize originally sequential order matching operations, minimizing data transfers between shared memory and HBM to achieve 99% HBM access efficiency.
            </p>

            <!-- ========== SCALING ========== -->
            <h2>Scaling to SPY500</h2>
            <p>
                Having established scaling laws on single-stock data, the next frontier is training on the <strong>full S&amp;P 500</strong> universe. The dataset scales are:
            </p>

            <table class="data-table" style="max-width: 450px; margin: 0 auto;">
                <thead>
                    <tr>
                        <th>Dataset</th>
                        <th>Tokens</th>
                        <th>Orders</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>GOOG 2022</td><td>8.3B</td><td>1/3B</td></tr>
                    <tr><td>GOOG (all years)</td><td>25B</td><td>1B</td></tr>
                    <tr><td><strong>SPY500</strong></td><td><strong>3,800B</strong></td><td><strong>152B</strong></td></tr>
                </tbody>
            </table>

            <p style="margin-top: 15px;">
                Scaling to the full SPY500 dataset with a 2B parameter model requires an estimated <strong>40,000&ndash;160,000 node-hours</strong>, derived from our single-stock training profile (16 node-hours/epoch for GOOG 2022), adjusted for the 456&times; data increase and ~50% multi-node communication efficiency.
            </p>
            <p>
                A multi-stock foundation model would unlock <strong>cross-stock correlation learning</strong>&mdash;understanding how order flow in one stock influences another&mdash;which is critical for portfolio-level applications.
            </p>

            <!-- ========== OPEN QUESTIONS ========== -->
            <h2>Open Questions</h2>
            <p>
                These are unsolved problems we are actively working on. If you have ideas, insights, or preliminary results &mdash; <strong>we'd love to hear from you</strong>. Promising contributions may lead to co-authorship on future publications. Click any question to explore the details and leave your thoughts below.
            </p>

            <!-- Q1: Attention -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q1</span> Quadratic Attention vs. Linear Attention vs. State Space Models?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Is financial order flow complex enough to require <strong>full quadratic attention</strong> ($O(n^2)$), or is the <strong>linear complexity</strong> ($O(n)$) of state space models sufficient? Transformers can capture arbitrary pairwise dependencies between tokens, while SSMs rely on compressed state representations. For LOB data&mdash;where long-range correlations may be weaker than in natural language&mdash;the answer is not obvious. Our scaling results with S5 are promising, but a rigorous comparison at 2B+ parameter scale remains open.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q1" data-term="Q1 Quadratic Attention vs. Linear Attention vs. State Space Models?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q2: Long Context -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q2</span> How Long Should the Context Window Be?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        With TBPTT=20 (10k orders), our context covers roughly <strong>3&ndash;30 minutes</strong> of real market time. Is this sufficient to capture the dynamics that matter for trading, or do we need hours or even full-day context? Longer windows capture intraday trends and slow-moving signals, but they increase computational cost and may introduce noise. The optimal context length likely depends on the downstream task&mdash;execution may need minutes, while portfolio optimization may need hours.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q2" data-term="Q2 How Long Should the Context Window Be?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q3: Tokenization -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q3</span> Tokenized Messages vs. Time-Stepped Representations?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        LOBS5 tokenizes each order message into 24 categorical tokens. An alternative is <strong>time-stepped representation</strong>&mdash;e.g., using RoPE (Rotary Position Embedding) with fixed time-step snapshots where each step aggregates multiple events. Tokenized representations preserve full message-level fidelity but produce very long sequences. Time-stepped approaches compress information but may lose critical microstructure details. Which is better for which tasks?
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q3" data-term="Q3 Tokenized Messages vs. Time-Stepped Representations?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q4: Data Scaling -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q4</span> Scale Up Stocks or Scale Up Years?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        For better <strong>test-time generalization</strong>, should we train on more stocks (cross-sectional diversity) or more years of the same stock (temporal diversity)? If train and test data come from the same distribution, scaling more stocks (SPY500) provides diversity across market microstructures, while scaling more years captures diverse market regimes (bull, bear, crisis). The optimal data scaling strategy for foundation model generalization in finance remains an open empirical question.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q4" data-term="Q4 Scale Up Stocks or Scale Up Years?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q5: Market Regime -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q5</span> Memorizing Market Regimes via Mixture of Experts?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        Financial markets exhibit distinct <strong>regimes</strong>&mdash;volatile vs. calm, trending vs. mean-reverting, crisis vs. normal. A single model must <strong>memorize</strong> all these regimes and switch behavior accordingly. Can a <strong>Mixture of Experts</strong> architecture achieve this by dedicating different experts to different regimes&mdash;where each expert memorizes the dynamics of a particular market state? The key questions are: how many distinct regimes need to be memorized, can a router learn to detect regime shifts in real time, and does explicit expert specialization outperform a single large model that must implicitly memorize all regimes in its parameters?
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q5" data-term="Q5 Memorizing Market Regimes via Mixture of Experts?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q6: Arithmetic Intensity -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q6</span> Low Arithmetic Intensity of State Space Models?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        SSMs have inherently <strong>low arithmetic intensity</strong>&mdash;the ratio of FLOPs to memory accesses is small, meaning they are memory-bandwidth-bound rather than compute-bound. This underutilizes <strong>Tensor Cores</strong> (designed for dense matrix multiply) and puts pressure on HBM bandwidth. How can we restructure SSM computations to increase arithmetic intensity? Possibilities include fusing more operations into single kernels, batching state updates, or hybrid architectures that mix attention (high intensity) with SSM (low intensity) layers.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q6" data-term="Q6 Low Arithmetic Intensity of State Space Models?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Q7: XLA Optimization -->
            <div class="oq-item">
                <button class="oq-btn" onclick="toggleOQ(this)">
                    <span class="oq-num">Q7</span> Hand-Written Kernels vs. XLA Auto-Optimization?
                </button>
                <div class="oq-body" style="display:none;">
                    <p>
                        JAX's <strong>XLA compiler</strong> is a black box that performs sophisticated graph-level optimizations (operator fusion, layout transformation, memory planning). Hand-written Triton kernels give precise control over memory access patterns, but XLA may find optimizations that humans miss. In practice, hand-optimized kernels are not guaranteed to be faster than XLA's auto-compiled output. When should we invest in custom kernels vs. trusting the compiler? A principled approach requires profiling to identify true bottlenecks before committing to manual optimization.
                    </p>
                    <div class="oq-comment-toggle">
                        <button onclick="loadComments(this)" style="font-size: 12px; color: #1772d0; background: none; border: none; cursor: pointer; padding: 5px 0;"><i class="fa fa-comment-o"></i> Comments</button>
                        <div class="oq-body" style="display:none;">
                            <div class="giscus-q7" data-term="Q7 Hand-Written Kernels vs. XLA Auto-Optimization?"></div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ========== SPONSOR ========== -->
            <h2>Support This Research</h2>
            <p>
                Scaling foundation models for finance requires substantial resources. If you or your organisation can help, we would be grateful for support in any of the following areas:
            </p>

            <table class="data-table" style="max-width: 600px; margin: 0 auto;">
                <thead>
                    <tr><th style="width: 30%;"><i class="fa fa-server"></i>&nbsp; Compute</th><th>What we need</th></tr>
                </thead>
                <tbody>
                    <tr><td>GPU / TPU clusters</td><td>Access to multi-node training infrastructure (H100, GH200, TPU v4+)</td></tr>
                    <tr><td>Cloud credits</td><td>AWS, GCP, Azure, or Lambda credits for scaling experiments</td></tr>
                    <tr><td>API tokens</td><td>Claude Code, Codex, or other AI coding assistant credits for research acceleration</td></tr>
                </tbody>
            </table>

            <table class="data-table" style="max-width: 600px; margin: 20px auto 0;">
                <thead>
                    <tr><th style="width: 30%;"><i class="fa fa-database"></i>&nbsp; Data</th><th>What we need</th></tr>
                </thead>
                <tbody>
                    <tr><td>Options markets</td><td>Full order book data for options exchanges (CBOE, CME, Eurex)</td></tr>
                    <tr><td>International equities</td><td>LOB data from non-US markets (LSE, TSE, SSE, HKEX, etc.)</td></tr>
                    <tr><td>Other asset classes</td><td>Futures, FX, or crypto order book data at message level</td></tr>
                </tbody>
            </table>

            <table class="data-table" style="max-width: 600px; margin: 20px auto 0;">
                <thead>
                    <tr><th style="width: 30%;"><i class="fa fa-graduation-cap"></i>&nbsp; Funding</th><th>What we need</th></tr>
                </thead>
                <tbody>
                    <tr><td>PhD studentships</td><td>Sponsorship for new doctoral researchers to join the project</td></tr>
                    <tr><td>Research grants</td><td>Collaborative grants or industry partnerships</td></tr>
                </tbody>
            </table>

            <p style="text-align: center; margin-top: 20px;">
                Interested? Reach out to <strong>Jakob Foerster</strong> at
                <a href="mailto:jakob.foerster@eng.ox.ac.uk">jakob.foerster@eng.ox.ac.uk</a>
            </p>

            <!-- ========== CITATION ========== -->
            <h2>Citation</h2>
            <div style="margin-top: 15px;">
                <textarea id="bibtex" style="display:none;">
@misc{lobs5_2025,
    title={Scaling Laws for Quantitative Finance Foundation Models},
    author={Ani Calinescu and Bidipta Sarkar and Reuben and Jakob Foerster and Kang Li and Mihai Cucuringu and Satyam and Gereon Franken and Peer Nagy and Stefan Zohren and Aramis Marti-Shahandeh and George Nigmatulin and Valentin and Sascha Frey and Frensi Zejnullahu and Alfie Backhouse},
    year={2025}
}
                </textarea>
            </div>

            <!-- ========== FOOTER ========== -->
            <div class="text-center" style="margin-top: 20px; margin-bottom: 30px; font-size: 12px; color: #999;">
                <p>
                    Template adapted from <a href="https://github.com/EasyAcademicWebsite/EasyAcademicWebsite.github.io">EasyAcademicWebsite</a>.
                </p>
            </div>

        </div><!-- col -->
    </div><!-- row -->
</div><!-- container -->

<!-- Scripts -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
<script src="utils/app.js"></script>

<!-- Dynamic author shuffle with seeded PRNG -->
<script>
(function() {
    // Seeded PRNG (mulberry32)
    function mulberry32(seed) {
        return function() {
            seed |= 0; seed = seed + 0x6D2B79F5 | 0;
            var t = Math.imul(seed ^ seed >>> 15, 1 | seed);
            t = t + Math.imul(t ^ t >>> 7, 61 | t) ^ t;
            return ((t ^ t >>> 14) >>> 0) / 4294967296;
        };
    }

    // Fisher-Yates shuffle with seeded random
    function shuffle(arr, rng) {
        var a = arr.slice();
        for (var i = a.length - 1; i > 0; i--) {
            var j = Math.floor(rng() * (i + 1));
            var tmp = a[i]; a[i] = a[j]; a[j] = tmp;
        }
        return a;
    }

    var faculty = new Set([
        'Stefan Zohren', 'Ani Calinescu', 'Jakob Foerster', 'Mihai Cucuringu',
        'Leandro S\u00e1nchez-Betancourt'
    ]);

    var allAuthors = [
        'Ani Calinescu', 'Bidipta Sarkar', 'Jakob Foerster',
        'Kang Li', 'Mihai Cucuringu', 'Satyam',
        'Stefan Zohren', 'Aramis Marti-Shahandeh',
        'George Nigmatulin', 'Valentin', 'Sascha Frey',
        'Leandro S\u00e1nchez-Betancourt'
    ];

    var now = new Date();
    var hhmmss = now.getHours() * 10000 + now.getMinutes() * 100 + now.getSeconds();
    var rng = mulberry32(hhmmss);

    // Split into students and faculty
    var students = allAuthors.filter(function(n) { return !faculty.has(n); });
    var facultyArr = allAuthors.filter(function(n) { return faculty.has(n); });

    // Shuffle students and faculty separately
    var shuffledStudents = shuffle(students, rng);
    var rng2 = mulberry32(hhmmss + 1);
    var shuffledFaculty = shuffle(facultyArr, rng2);

    // Render student list with wide spacing (like eshyperscale)
    document.getElementById('author-list').innerHTML =
        shuffledStudents.map(function(n) {
            return '<span style="display:inline-block; margin: 0 12px;">' + n + '</span>';
        }).join('');

    // Render faculty list, mark Jakob Foerster as PI
    document.getElementById('faculty-list').innerHTML =
        '<em>Faculty: ' + shuffledFaculty.map(function(n) {
            return n === 'Jakob Foerster' ? n + ' (PI)' : n;
        }).join(', ') + '</em>';

    // Seed note right below authors
    var pad = function(n) { return n < 10 ? '0' + n : '' + n; };
    var timeStr = pad(now.getHours()) + ':' + pad(now.getMinutes()) + ':' + pad(now.getSeconds());
    document.getElementById('seed-note').innerHTML =
        '<sup>&dagger;</sup> Author order randomized with <code>seed(' + hhmmss + ')</code> &mdash; ' +
        'current timestamp HHMMSS (' + timeStr + '). Reload to re-shuffle.';
})();
</script>

<!-- Open Questions toggle -->
<script>
function toggleOQ(btn) {
    var body = btn.nextElementSibling;
    if (!body) return;
    body.style.display = body.style.display === 'none' ? 'block' : 'none';
}

// Single shared giscus widget - moved between questions
var giscusLoaded = false;
var giscusContainer = document.createElement('div');
giscusContainer.id = 'shared-giscus';
giscusContainer.style.marginTop = '12px';

function loadComments(btn) {
    var container = btn.nextElementSibling;
    if (!container) return;

    // If clicking same question again, toggle off
    if (container.contains(giscusContainer) && container.style.display === 'block') {
        container.style.display = 'none';
        return;
    }

    // Hide any previously open comment containers
    document.querySelectorAll('.oq-comment-toggle .oq-body').forEach(function(el) {
        el.style.display = 'none';
    });

    container.style.display = 'block';
    var gDiv = container.querySelector('[data-term]');
    if (!gDiv) return;
    var term = gDiv.dataset.term;

    // Move shared container into this question
    gDiv.innerHTML = '';
    gDiv.appendChild(giscusContainer);

    if (!giscusLoaded) {
        // First load: create the giscus widget
        var script = document.createElement('script');
        script.src = 'https://giscus.app/client.js';
        script.setAttribute('data-repo', 'KangOxford/sigma0');
        script.setAttribute('data-repo-id', 'R_kgDORKDiSQ');
        script.setAttribute('data-category', 'General');
        script.setAttribute('data-category-id', 'DIC_kwDORKDiSc4C1-T6');
        script.setAttribute('data-mapping', 'specific');
        script.setAttribute('data-term', term);
        script.setAttribute('data-strict', '0');
        script.setAttribute('data-reactions-enabled', '1');
        script.setAttribute('data-emit-metadata', '0');
        script.setAttribute('data-input-position', 'bottom');
        script.setAttribute('data-theme', 'light');
        script.setAttribute('data-lang', 'en');
        script.setAttribute('crossorigin', 'anonymous');
        script.async = true;
        giscusContainer.appendChild(script);
        giscusLoaded = true;
    } else {
        // Already loaded: use postMessage to switch Discussion
        var iframe = document.querySelector('iframe.giscus-frame');
        if (iframe) {
            iframe.contentWindow.postMessage(
                { giscus: { setConfig: { term: term } } },
                'https://giscus.app'
            );
        }
    }
}

// Watch for giscus iframe insertion and scale it down
var giscusObserver = new MutationObserver(function() {
    var iframe = document.querySelector('#shared-giscus iframe.giscus-frame');
    if (iframe && !iframe.dataset.scaled) {
        iframe.style.transform = 'scale(0.75)';
        iframe.style.transformOrigin = 'top left';
        iframe.style.width = '133%';
        iframe.dataset.scaled = '1';
    }
});
giscusObserver.observe(giscusContainer, { childList: true, subtree: true });
</script>

<!-- Collapsible toggle -->
<script>
document.querySelectorAll('.collapsible').forEach(function(btn) {
    btn.addEventListener('click', function() {
        this.classList.toggle('active');
        var content = this.nextElementSibling;
        content.classList.toggle('show');
    });
});
</script>

</body>
</html>
